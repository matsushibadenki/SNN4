# matsushibadenki/snn3/train.py
# (æ›´æ–°)
# æ–°ã—ã„çµ±åˆå­¦ç¿’å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ (å®Œå…¨ç‰ˆ)
#
# ä¿®æ­£(mypy): [annotation-unchecked] noteã‚’è§£æ¶ˆã™ã‚‹ãŸã‚ã€å‹ãƒ’ãƒ³ãƒˆã‚’è¿½åŠ ã€‚
# ä¿®æ­£(mypy): [name-defined]ã‚¨ãƒ©ãƒ¼ã‚’è§£æ¶ˆã™ã‚‹ãŸã‚ã€Unionã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã€‚
# ä¿®æ­£(mypy): [union-attr]ã‚¨ãƒ©ãƒ¼ã‚’è§£æ¶ˆã™ã‚‹ãŸã‚ã€trainerã®ç¨®é¡ã«å¿œã˜ã¦å‡¦ç†ã‚’åˆ†å²ã€‚

import argparse
import os
import torch
import torch.distributed as dist
import torch.nn as nn
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, random_split, DistributedSampler
from dependency_injector.wiring import inject, Provide
from typing import Optional, Tuple, List, Dict, Any, Callable, cast, Union
from transformers import PreTrainedTokenizerBase

from app.containers import TrainingContainer
from snn_research.data.datasets import get_dataset_class, DistillationDataset, DataFormat, SNNBaseDataset
from snn_research.training.trainers import BreakthroughTrainer, ParticleFilterTrainer
from snn_research.training.bio_trainer import BioRLTrainer
from snn_research.training.quantization import apply_qat, convert_to_quantized_model
from snn_research.training.pruning import apply_magnitude_pruning
from scripts.data_preparation import prepare_wikitext_data
from snn_research.core.snn_core import SNNCore
from app.utils import get_auto_device


# DIã‚³ãƒ³ãƒ†ãƒŠã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
container = TrainingContainer()

@inject
def train(
    args: argparse.Namespace,
    config: Dict[str, Any] = Provide[TrainingContainer.config],
    tokenizer: PreTrainedTokenizerBase = Provide[TrainingContainer.tokenizer],
) -> None:
    """å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ã‚’å®Ÿè¡Œã™ã‚‹ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    is_distributed = args.distributed
    rank = int(os.environ.get("LOCAL_RANK", -1))
    device = f'cuda:{rank}' if is_distributed and torch.cuda.is_available() else get_auto_device()
    
    paradigm = config['training']['paradigm']

    print(f"ğŸš€ å­¦ç¿’ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ  '{paradigm}' ã§å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™...")
    
    trainer: Union[BreakthroughTrainer, BioRLTrainer, ParticleFilterTrainer]

    if paradigm.startswith("bio-"):
        # --- ç”Ÿç‰©å­¦çš„å­¦ç¿’ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã®å®Ÿè¡Œ ---
        if paradigm == "bio-causal-sparse":
            print("ğŸ§¬ é©å¿œçš„å› æœã‚¹ãƒ‘ãƒ¼ã‚¹åŒ–ã‚’æœ‰åŠ¹ã«ã—ãŸå¼·åŒ–å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™ã€‚")
            container.config.training.biologically_plausible.adaptive_causal_sparsification.enabled.from_value(True)
            trainer = container.bio_rl_trainer()
            cast(BioRLTrainer, trainer).train(num_episodes=config['training']['epochs'])
        elif paradigm == "bio-particle-filter":
            print("ğŸŒªï¸ ãƒ‘ãƒ¼ãƒ†ã‚£ã‚¯ãƒ«ãƒ•ã‚£ãƒ«ã‚¿ã«ã‚ˆã‚‹ç¢ºç‡çš„å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™ (CPUãƒ™ãƒ¼ã‚¹)ã€‚")
            container.config.training.biologically_plausible.particle_filter.enabled.from_value(True)
            trainer = container.particle_filter_trainer()
            dummy_data = torch.rand(1, 10, device=device)
            dummy_targets = torch.rand(1, 2, device=device)
            for epoch in range(config['training']['epochs']):
                loss = cast(ParticleFilterTrainer, trainer).train_step(dummy_data, dummy_targets)
                print(f"Epoch {epoch+1}/{config['training']['epochs']}: Particle Filter Loss = {loss:.4f}")
        elif paradigm == "bio-probabilistic-hebbian":
            print("ğŸ§¬ ç¢ºç‡çš„ãƒ˜ãƒ–å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™...")
            # DIã‚³ãƒ³ãƒ†ãƒŠã‹ã‚‰ç¢ºç‡çš„ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’å–å¾—
            prob_trainer: BioRLTrainer = container.probabilistic_trainer()
            # è«–æ–‡ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯æ•™å¸«ãªã—å­¦ç¿’ã®å¯èƒ½æ€§ãŒé«˜ã„ãŸã‚ã€
            # RLãƒ«ãƒ¼ãƒ—ã§ã¯ãªãã€ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦
            # ãƒ¢ãƒ‡ãƒ«ã® update_weights ã‚’ç›´æ¥å‘¼ã³å‡ºã™å½¢ã«ãªã‚‹ã‹ã‚‚ã—ã‚Œãªã„
            # ã“ã“ã§ã¯ BioRLTrainer ã‚’æµç”¨ã™ã‚‹å‰æã§ train ã‚’å‘¼ã³å‡ºã™
            prob_trainer.train(num_episodes=config['training']['epochs'])
        else:
            raise ValueError(f"ä¸æ˜ãªç”Ÿç‰©å­¦çš„å­¦ç¿’ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ : {paradigm}")

    elif paradigm in ["gradient_based", "self_supervised", "physics_informed", "probabilistic_ensemble"]:
        # --- å‹¾é…ãƒ™ãƒ¼ã‚¹å­¦ç¿’ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã®å®Ÿè¡Œ ---
        if is_distributed and paradigm != "gradient_based":
            raise NotImplementedError(f"{paradigm} learning does not support DDP yet.")

        is_distillation = paradigm == "gradient_based" and config['training']['gradient_based']['type'] == "distillation"
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™
        wikitext_path = "data/wikitext-103_train.jsonl"
        if os.path.exists(wikitext_path):
            data_path = wikitext_path
        else:
            data_path = args.data_path or config['data']['path']
        
        DatasetClass = get_dataset_class(DataFormat(config['data']['format']))
        dataset: SNNBaseDataset
        if is_distillation:
            dataset = DistillationDataset(file_path=os.path.join(data_path, "distillation_data.jsonl"), data_dir=data_path, tokenizer=tokenizer, max_seq_len=config['model']['time_steps'])
        else:
            dataset = DatasetClass(file_path=data_path, tokenizer=tokenizer, max_seq_len=config['model']['time_steps'])
            
        train_size = int((1.0 - config['data']['split_ratio']) * len(dataset))
        val_size = len(dataset) - train_size
        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

        train_sampler: Optional[DistributedSampler] = DistributedSampler(train_dataset) if is_distributed else None
        train_loader = DataLoader(train_dataset, batch_size=config['training']['batch_size'], shuffle=(train_sampler is None), sampler=train_sampler, collate_fn=collate_fn(tokenizer, is_distillation))
        val_loader = DataLoader(val_dataset, batch_size=config['training']['batch_size'], shuffle=False, collate_fn=collate_fn(tokenizer, is_distillation))

        snn_model: nn.Module = container.snn_model(backend=args.backend)

        if config.get('training', {}).get('quantization', {}).get('enabled', False):
            snn_model = apply_qat(snn_model.to('cpu'))
        snn_model.to(device)

        if is_distributed:
            snn_model = DDP(snn_model, device_ids=[rank], find_unused_parameters=True)
        
        astrocyte = container.astrocyte_network(snn_model=snn_model) if args.use_astrocyte else None
        
        trainer_provider: Callable[..., BreakthroughTrainer]
        optimizer: torch.optim.Optimizer
        scheduler: Optional[torch.optim.lr_scheduler.LRScheduler]

        if paradigm == "gradient_based":
            optimizer = container.optimizer(params=snn_model.parameters())
            scheduler = container.scheduler(optimizer=optimizer) if config['training']['gradient_based']['use_scheduler'] else None
            trainer_provider = container.distillation_trainer if is_distillation else container.standard_trainer
        elif paradigm == "self_supervised":
            optimizer = container.optimizer(params=snn_model.parameters())
            scheduler = container.scheduler(optimizer=optimizer) if config['training']['self_supervised']['use_scheduler'] else None
            trainer_provider = container.self_supervised_trainer
        elif paradigm == "physics_informed":
            optimizer = container.pi_optimizer(params=snn_model.parameters())
            scheduler = container.pi_scheduler(optimizer=optimizer) if config['training']['physics_informed']['use_scheduler'] else None
            trainer_provider = container.physics_informed_trainer
        else: # probabilistic_ensemble
            optimizer = container.optimizer(params=snn_model.parameters())
            scheduler = container.scheduler(optimizer=optimizer) if config['training']['probabilistic_ensemble']['use_scheduler'] else None
            trainer_provider = container.probabilistic_ensemble_trainer

        trainer = trainer_provider(model=snn_model, optimizer=optimizer, scheduler=scheduler, device=device, rank=rank, astrocyte_network=astrocyte)

        if args.load_ewc_data:
            trainer.load_ewc_data(args.load_ewc_data)

        start_epoch = trainer.load_checkpoint(args.resume_path) if args.resume_path else 0
        for epoch in range(start_epoch, config['training']['epochs']):
            if train_sampler: train_sampler.set_epoch(epoch)
            trainer.train_epoch(train_loader, epoch)
            if rank in [-1, 0] and (epoch % config['training']['eval_interval'] == 0 or epoch == config['training']['epochs'] - 1):
                val_metrics = trainer.evaluate(val_loader, epoch)
                if epoch % config['training']['log_interval'] == 0:
                    checkpoint_path = os.path.join(config['training']['log_dir'], f"checkpoint_epoch_{epoch}.pth")
                    trainer.save_checkpoint(path=checkpoint_path, epoch=epoch, metric_value=val_metrics.get('total', float('inf')), tokenizer_name=config['data']['tokenizer_name'], config=config['model'])
        
        if rank in [-1, 0] and args.task_name and config['training']['gradient_based']['loss']['ewc_weight'] > 0:
            trainer._compute_ewc_fisher_matrix(train_loader, args.task_name)
            
        # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã®å‡¦ç† (é‡å­åŒ–ã€ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°)
        if rank in [-1, 0]:
            final_model = trainer.model.module if is_distributed else trainer.model
            if isinstance(final_model, nn.Module): # å‹ã‚¬ãƒ¼ãƒ‰
                if config.get('training', {}).get('quantization', {}).get('enabled', False):
                    quantized_model = convert_to_quantized_model(final_model.to('cpu'))
                    quantized_path = os.path.join(config['training']['log_dir'], 'quantized_best_model.pth')
                    torch.save(quantized_model.state_dict(), quantized_path)
                if config.get('training', {}).get('pruning', {}).get('enabled', False):
                    pruning_amount = config['training']['pruning'].get('amount', 0.2)
                    pruned_model = apply_magnitude_pruning(final_model, amount=pruning_amount)
                    pruned_path = os.path.join(config['training']['log_dir'], 'pruned_best_model.pth')
                    torch.save(pruned_model.state_dict(), pruned_path)

    else:
        raise ValueError(f"Unknown training paradigm: '{paradigm}'.")

    print("âœ… å­¦ç¿’ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")


def collate_fn(tokenizer: PreTrainedTokenizerBase, is_distillation: bool) -> Callable[[List[Tuple[torch.Tensor, ...]]], Tuple[torch.Tensor, ...]]:
    def collate(batch: List[Tuple[torch.Tensor, ...]]) -> Tuple[torch.Tensor, ...]:
        padding_val = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0
        
        inputs = [item[0] for item in batch]
        targets = [item[1] for item in batch]
        
        padded_inputs = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=float(padding_val))
        padded_targets = torch.nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=float(padding_val))

        if is_distillation:
            logits = [item[2] for item in batch]
            padded_logits = torch.nn.utils.rnn.pad_sequence(logits, batch_first=True, padding_value=0.0)
            return padded_inputs.long(), padded_targets.long(), padded_logits
        return padded_inputs.long(), padded_targets.long()
    return collate
    
def main() -> None:
    parser = argparse.ArgumentParser(description="SNN çµ±åˆå­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    parser.add_argument("--config", type=str, default="configs/base_config.yaml", help="åŸºæœ¬è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«")
    parser.add_argument("--model_config", type=str, help="ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«")
    parser.add_argument("--data_path", type=str, help="ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ‘ã‚¹ï¼ˆconfigã‚’ä¸Šæ›¸ãï¼‰")
    parser.add_argument("--task_name", type=str, help="EWCã®ãŸã‚ã«ã‚¿ã‚¹ã‚¯åã‚’æŒ‡å®š (ä¾‹: 'sst2')")
    parser.add_argument("--override_config", type=str, action='append', help="è¨­å®šã‚’ä¸Šæ›¸ã (ä¾‹: 'training.epochs=5')")
    parser.add_argument("--distributed", action="store_true", help="åˆ†æ•£å­¦ç¿’ã‚’æœ‰åŠ¹ã«ã™ã‚‹")
    parser.add_argument("--resume_path", type=str, help="ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å­¦ç¿’ã‚’å†é–‹ã™ã‚‹")
    parser.add_argument("--load_ewc_data", type=str, help="äº‹å‰è¨ˆç®—ã•ã‚ŒãŸEWCã®Fisherè¡Œåˆ—ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ‘ã‚¹")
    parser.add_argument("--use_astrocyte", action="store_true", help="ã‚¢ã‚¹ãƒˆãƒ­ã‚µã‚¤ãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æœ‰åŠ¹ã«ã™ã‚‹ (gradient_basedç³»ã®ã¿)")
    parser.add_argument("--paradigm", type=str, help="å­¦ç¿’ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚’ä¸Šæ›¸ã (ä¾‹: gradient_based, bio-causal-sparse, bio-particle-filter)")
    parser.add_argument("--backend", type=str, default="spikingjelly", choices=["spikingjelly", "snntorch"], help="SNNã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ–ãƒ©ãƒª")
    args = parser.parse_args()

    container.config.from_yaml(args.config)
    if args.model_config: container.config.from_yaml(args.model_config)
    if args.data_path: container.config.data.path.from_value(args.data_path)
    if args.paradigm: container.config.training.paradigm.from_value(args.paradigm)
    
    if args.override_config:
        for override in args.override_config:
            keys, value_str = override.split('=', 1)
            try:
                value: Any = int(value_str)
            except ValueError:
                try:
                    value = float(value_str)
                except ValueError:
                    if value_str.lower() == 'true':
                        value = True
                    elif value_str.lower() == 'false':
                        value = False
                    else:
                        value = value_str
            
            config_dict: Dict[str, Any] = {}
            temp_dict = config_dict
            key_parts = keys.split('.')
            for i, part in enumerate(key_parts):
                if i == len(key_parts) - 1:
                    temp_dict[part] = value
                else:
                    temp_dict[part] = {}
                    temp_dict = temp_dict[part]
            container.config.from_dict(config_dict)

    if args.distributed: dist.init_process_group(backend="nccl")
    
    container.wire(modules=[__name__])
    
    injected_config = container.config()
    injected_tokenizer = container.tokenizer()
    train(args, config=injected_config, tokenizer=injected_tokenizer)
    
    if args.distributed: dist.destroy_process_group()

if __name__ == "__main__":
    main()
