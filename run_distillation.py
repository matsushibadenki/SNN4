# matsushibadenki/snn4/run_distillation.py
# Title: çŸ¥è­˜è’¸ç•™å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
# Description: KnowledgeDistillationManagerã‚’ä½¿ç”¨ã—ã¦ã€çŸ¥è­˜è’¸ç•™ãƒ—ãƒ­ã‚»ã‚¹ã‚’é–‹å§‹ã—ã¾ã™ã€‚
#              è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã¨ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚
#              mypyã‚¨ãƒ©ãƒ¼ä¿®æ­£: Containerã‚’TrainingContainerã«ä¿®æ­£ã€‚
# æ”¹å–„ç‚¹: argparseã‚’è¿½åŠ ã—ã€asyncio.runã§å®Ÿè¡Œã™ã‚‹ã‚ˆã†ã«ä¿®æ­£ã€‚
# æ”¹å–„ç‚¹(snn_4_ann_parity_plan):
# - ANNæ•™å¸«ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã€AutoModelForCausalLMã®ä»£ã‚ã‚Šã«å…·ä½“çš„ãªANNBaselineModelã‚’
#   ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã™ã‚‹ã‚ˆã†ã«ä¿®æ­£ã—ã€ã‚ˆã‚Šç®¡ç†ã•ã‚ŒãŸè’¸ç•™ãƒ—ãƒ­ã‚»ã‚¹ã‚’å®Ÿç¾ã€‚

import argparse
import asyncio
from app.containers import TrainingContainer
from snn_research.distillation.knowledge_distillation_manager import KnowledgeDistillationManager
from snn_research.benchmark.ann_baseline import ANNBaselineModel

async def main():
    parser = argparse.ArgumentParser(description="SNN Knowledge Distillation Runner")
    parser.add_argument("--config", type=str, default="configs/base_config.yaml", help="Base config file path")
    parser.add_argument("--model_config", type=str, default="configs/models/small.yaml", help="Model architecture config file path")
    args = parser.parse_args()

    # DIã‚³ãƒ³ãƒ†ãƒŠã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
    container = TrainingContainer()
    container.config.from_yaml(args.config)
    container.config.from_yaml(args.model_config)

    # --- â–¼ ä¿®æ­£ â–¼ ---
    # DIã‚³ãƒ³ãƒ†ãƒŠã‹ã‚‰å¿…è¦ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’æ­£ã—ã„é †åºã§å–å¾—ãƒ»æ§‹ç¯‰
    device = container.device()
    student_model = container.snn_model().to(device)
    optimizer = container.optimizer(params=student_model.parameters())
    scheduler = container.scheduler(optimizer=optimizer) if container.config.training.gradient_based.use_scheduler() else None

    # --- â–¼ snn_4_ann_parity_planã«åŸºã¥ãä¿®æ­£ â–¼ ---
    # æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ANNBaselineModelã‚’æ˜ç¤ºçš„ã«æ§‹ç¯‰
    print("ğŸ§  Initializing ANN teacher model (ANNBaselineModel)...")
    snn_config = container.config.model.to_dict()
    teacher_model = ANNBaselineModel(
        vocab_size=container.tokenizer.provided.vocab_size(),
        d_model=snn_config.get('d_model', 128),
        nhead=snn_config.get('n_head', 2),
        d_hid=snn_config.get('d_model', 128) * 4, # ä¸€èˆ¬çš„ãªFFNã®æ‹¡å¼µç‡
        nlayers=snn_config.get('num_layers', 4),
        num_classes=container.tokenizer.provided.vocab_size()
    ).to(device)
    # æ³¨: å®Ÿéš›ã®ä½¿ç”¨ä¾‹ã§ã¯ã€ã“ã“ã§æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’æ¸ˆã¿é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™
    # teacher_model.load_state_dict(torch.load("path/to/teacher.pth"))
    # --- â–² snn_4_ann_parity_planã«åŸºã¥ãä¿®æ­£ â–² ---

    distillation_trainer = container.distillation_trainer(
        model=student_model,
        optimizer=optimizer,
        scheduler=scheduler,
        device=device
    )
    model_registry = container.model_registry()

    manager = KnowledgeDistillationManager(
        student_model=student_model,
        # teacher_model_nameã®ä»£ã‚ã‚Šã«ã€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã•ã‚ŒãŸæ•™å¸«ãƒ¢ãƒ‡ãƒ«ã‚’æ¸¡ã™ã‚ˆã†ã«å¤‰æ›´
        teacher_model=teacher_model,
        trainer=distillation_trainer,
        tokenizer_name=container.config.data.tokenizer_name(),
        model_registry=model_registry,
        device=device
    )
    # --- â–² ä¿®æ­£ â–² ---

    # (ä»®) ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™
    # å®Ÿéš›ã«ã¯ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹
    sample_texts = [
        "Spiking Neural Networks are a promising alternative to traditional ANNs.",
        "They operate based on discrete events, which can lead to greater energy efficiency.",
        "Knowledge distillation is a technique to transfer knowledge from a large model to a smaller one."
    ]
    train_loader = manager.prepare_dataset(
        sample_texts,
        max_length=container.config.model.time_steps(),
        batch_size=container.config.training.batch_size()
    )
    val_loader = train_loader # ç°¡å˜ã®ãŸã‚åŒã˜ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨

    # è’¸ç•™ã®å®Ÿè¡Œ
    await manager.run_distillation(
        train_loader=train_loader,
        val_loader=val_loader,
        epochs=3, # ãƒ†ã‚¹ãƒˆç”¨ã®ã‚¨ãƒãƒƒã‚¯æ•°
        model_id="distilled_snn_expert_v1",
        task_description="An expert SNN for explaining AI concepts, created via distillation.",
        student_config=container.config.model.to_dict()
    )

if __name__ == "__main__":
    asyncio.run(main())
