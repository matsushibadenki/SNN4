# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/distillation/knowledge_distillation_manager.py
# ã‚³ãƒ¼ãƒ‰ã®æœ€ã‚‚æœ€åˆã«ã¯ã€ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã€ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’ç¤ºã—ãŸã‚¿ã‚¤ãƒˆãƒ«ã€æ©Ÿèƒ½ã®èª¬æ˜ã‚’è©³ç´°ã«è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚ ä¿®æ­£å†…å®¹ã¯è¨˜è¼‰ã™ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
# ã‚¿ã‚¤ãƒˆãƒ«: çŸ¥è­˜è’¸ç•™ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
# æ©Ÿèƒ½èª¬æ˜: çŸ¥è­˜è’¸ç•™ãƒ—ãƒ­ã‚»ã‚¹ã‚’çµ±æ‹¬ã™ã‚‹ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚¯ãƒ©ã‚¹ã€‚
# BugFix: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå´ã§å…¥åŠ›ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®ãƒšã‚¢ã‚’æ­£ã—ãä½œæˆã™ã‚‹ã‚ˆã†ã«ä¿®æ­£ã—ã€
#         collate_fnã‚’ç°¡ç´ åŒ–ã™ã‚‹ã“ã¨ã§ã€å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ä¸æ•´åˆå•é¡Œã‚’è§£æ¶ˆã€‚
# BugFix: ãƒ•ã‚¡ã‚¤ãƒ«å†…ã«ã‚ã£ãŸä¸æ­£ãªé–‰ã˜æ‹¬å¼§ã‚’å‰Šé™¤ã—ã€mypyã®æ§‹æ–‡ã‚¨ãƒ©ãƒ¼ã‚’ä¿®æ­£ã€‚
# ä¿®æ­£: mypyã‚¨ãƒ©ãƒ¼ `Name "Tuple" is not defined` ã‚’è§£æ¶ˆã™ã‚‹ãŸã‚ã€Tupleã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã€‚
# ä¿®æ­£(mypy): [annotation-unchecked] noteã‚’è§£æ¶ˆã™ã‚‹ãŸã‚ã€å†…éƒ¨ã‚¯ãƒ©ã‚¹ãƒ»é–¢æ•°ã®
#             å‹ãƒ’ãƒ³ãƒˆã‚’ä¿®æ­£ãƒ»è¿½åŠ ã€‚
# æ”¹å–„ç‚¹(v2): ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™ãƒ­ã‚¸ãƒƒã‚¯ã‚’æ±ç”¨åŒ–ã—ã€ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã‚‚å¯¾å¿œã€‚
# ä¿®æ­£(v3): mypyã‚¨ãƒ©ãƒ¼ [arg-type] ã‚’è§£æ¶ˆã™ã‚‹ãŸã‚ã€castã‚’ä½¿ç”¨ã—ã¦å‹ã‚’æ˜ç¤ºã€‚
# æ”¹å–„ç‚¹(v4): ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾å¿œã™ã‚‹ã‚ˆã† prepare_dataset ã¨ distillation_collate_fn ã‚’ä¿®æ­£ã€‚

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from transformers import PreTrainedModel, PreTrainedTokenizer, AutoModelForCausalLM, AutoTokenizer, PreTrainedTokenizerBase
from typing import Dict, Any, Optional, List, TYPE_CHECKING, cast, Tuple, Callable, Sized
import asyncio
import os
import json
from tqdm import tqdm
from omegaconf import OmegaConf

from snn_research.distillation.model_registry import ModelRegistry
from snn_research.benchmark.metrics import calculate_perplexity, calculate_energy_consumption
from snn_research.core.snn_core import SNNCore

# --- å¾ªç’°ã‚¤ãƒ³ãƒãƒ¼ãƒˆè§£æ¶ˆã®ãŸã‚ã®ä¿®æ­£ ---
# å‹ãƒã‚§ãƒƒã‚¯æ™‚ã®ã¿ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’å®Ÿè¡Œã—ã€å®Ÿè¡Œæ™‚ã®å¾ªç’°å‚ç…§ã‚’å›é¿ã™ã‚‹
if TYPE_CHECKING:
    from snn_research.training.trainers import DistillationTrainer

class KnowledgeDistillationManager:
    """
    çŸ¥è­˜è’¸ç•™ãƒ—ãƒ­ã‚»ã‚¹ã‚’çµ±æ‹¬ã™ã‚‹ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚¯ãƒ©ã‚¹ã€‚
    """
    def __init__(
        self,
        student_model: torch.nn.Module,
        trainer: "DistillationTrainer",
        tokenizer_name: str,
        model_registry: ModelRegistry,
        device: str,
        teacher_model: Optional[torch.nn.Module] = None,
        teacher_model_name: Optional[str] = None
    ):
        self.student_model = student_model.to(device)
        self.distillation_trainer = trainer

        if teacher_model is not None:
            self.teacher_model = teacher_model.to(device)
        elif teacher_model_name is not None:
            self.teacher_model = AutoModelForCausalLM.from_pretrained(teacher_model_name).to(device)
        else:
            raise ValueError("Either teacher_model or teacher_model_name must be provided.")

        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        self.model_registry = model_registry
        self.device = device

    def prepare_dataset(
        self,
        train_dataset: Dataset,
        val_dataset: Dataset,
        collate_fn: Callable, # å…ƒã®ã‚¿ã‚¹ã‚¯ã®collate_fnã‚’å—ã‘å–ã‚‹
        batch_size: int
    ) -> Tuple[DataLoader, DataLoader]:
        """
        æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ©ãƒƒãƒ—ã—ã€æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’å‹•çš„ã«ä»˜ä¸ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’æº–å‚™ã™ã‚‹ã€‚
        ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã‚‚å¯¾å¿œã€‚
        """
        class _DistillationWrapperDataset(Dataset):
            def __init__(self, original_dataset: Dataset, teacher_model: nn.Module, device: str):
                self.original_dataset = original_dataset
                self.teacher_model = teacher_model
                self.device = device

            def __len__(self) -> int:
                return len(cast(Sized, self.original_dataset))

            @torch.no_grad()
            def __getitem__(self, idx: int) -> Dict[str, Any]:
                item = self.original_dataset[idx] # original_datasetãŒè¿”ã™å½¢å¼ (ç”»åƒãªã‚‰(img, label)ã€ãƒ†ã‚­ã‚¹ãƒˆãªã‚‰(input_ids, target_ids)ãªã©)

                # --- â–¼ ç”»åƒ/ãƒ†ã‚­ã‚¹ãƒˆåˆ¤å®šã¨å…¥åŠ›å½¢å¼çµ±ä¸€ â–¼ ---
                if isinstance(item[0], torch.Tensor) and item[0].ndim >= 2: # ç”»åƒãƒ‡ãƒ¼ã‚¿ã¨ä»®å®š (ä¾‹: [C, H, W])
                    inputs = item[0].unsqueeze(0).to(self.device) # ãƒãƒƒãƒæ¬¡å…ƒã‚’è¿½åŠ ã—ã¦ãƒ‡ãƒã‚¤ã‚¹ã¸
                    label = item[1]
                    input_key = "input_images"
                elif isinstance(item[0], torch.Tensor) and item[0].ndim == 1: # ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ (input_ids) ã¨ä»®å®š
                    inputs = item[0].unsqueeze(0).to(self.device)
                    label = item[1] # target_ids
                    input_key = "input_ids"
                else:
                    raise TypeError(f"Unsupported data type from original_dataset: {type(item[0])}")
                # --- â–² ç”»åƒ/ãƒ†ã‚­ã‚¹ãƒˆåˆ¤å®šã¨å…¥åŠ›å½¢å¼çµ±ä¸€ â–² ---

                # æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã§ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®—
                teacher_output = self.teacher_model(inputs)
                # ç”»åƒãƒ¢ãƒ‡ãƒ«ã¯ã‚¿ãƒ—ãƒ«ã‚’è¿”ã•ãªã„å ´åˆãŒã‚ã‚‹
                teacher_logits_batch = (teacher_output.logits if hasattr(teacher_output, 'logits') else teacher_output)
                # ãƒãƒƒãƒæ¬¡å…ƒã‚’å‰Šé™¤ã—ã¦CPUã¸
                teacher_logits = teacher_logits_batch.squeeze(0).cpu()

                # collate_fnã§æ‰±ãˆã‚‹è¾æ›¸å½¢å¼ã§è¿”ã™
                return {"inputs": item[0], "labels": label, "teacher_logits": teacher_logits, "input_key": input_key}

        train_wrapper = _DistillationWrapperDataset(train_dataset, self.teacher_model, self.device)
        val_wrapper = _DistillationWrapperDataset(val_dataset, self.teacher_model, self.device)

        def distillation_collate_fn(batch: List[Dict[str, Any]]) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
            """
            DistillationWrapperDatasetã‹ã‚‰ã®è¾æ›¸ã‚’ãƒãƒƒãƒåŒ–ã—ã€
            DistillationTrainerãŒæœŸå¾…ã™ã‚‹ã‚¿ãƒ—ãƒ«å½¢å¼ (student_input, attention_mask, student_target, teacher_logits) ã«å¤‰æ›ã™ã‚‹ã€‚
            """
            input_key = batch[0]["input_key"] # ãƒãƒƒãƒå†…ã®input_keyã¯åŒã˜ã¨ä»®å®š

            if input_key == "input_images":
                student_input = torch.stack([item['inputs'] for item in batch])
                # ç”»åƒã®å ´åˆã€attention_maskã¯é€šå¸¸ä¸è¦ã ãŒã€Trainerã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã«åˆã‚ã›ã¦ãƒ€ãƒŸãƒ¼ã‚’ä½œæˆ
                attention_mask = torch.ones(student_input.shape[0], student_input.shape[-1], dtype=torch.long) # ç”»åƒã‚µã‚¤ã‚ºã«åˆã‚ã›ãŸãƒ€ãƒŸãƒ¼ãƒã‚¹ã‚¯ (ã‚ˆã‚Šè‰¯ã„æ–¹æ³•ãŒã‚ã‚Œã°ä¿®æ­£)
                student_target = torch.tensor([item['labels'] for item in batch], dtype=torch.long) # åˆ†é¡ãƒ©ãƒ™ãƒ«
                teacher_logits = torch.stack([item['teacher_logits'] for item in batch]) # åˆ†é¡ãƒ­ã‚¸ãƒƒãƒˆ
            elif input_key == "input_ids":
                # å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆcollate_fnã‚’ä½¿ç”¨ã—ã¦input_idsã¨attention_maskã‚’ä½œæˆ
                # ãŸã ã—ã€å…ƒã®collate_fnã¯ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆã‚’æœŸå¾…ã™ã‚‹ãŸã‚ã€å†æ§‹ç¯‰ãŒå¿…è¦
                original_batch_for_collate = [{"text": self.tokenizer.decode(item['inputs']), "label": item['labels']} for item in batch] # ãƒ©ãƒ™ãƒ«ã¯ãƒ€ãƒŸãƒ¼
                collated_original = collate_fn(original_batch_for_collate) # å…ƒã®collate_fnã‚’å‘¼ã³å‡ºã—

                student_input = collated_original['input_ids']
                attention_mask = collated_original['attention_mask']
                student_target = torch.nn.utils.rnn.pad_sequence(
                    [item['labels'] for item in batch], batch_first=True, padding_value=self.tokenizer.pad_token_id or 0
                )
                teacher_logits = torch.nn.utils.rnn.pad_sequence(
                    [item['teacher_logits'] for item in batch], batch_first=True, padding_value=0.0
                )
                # Ensure teacher_logits has the same seq_len as student_input
                target_len = student_input.shape[1]
                current_len = teacher_logits.shape[1]
                if current_len < target_len:
                    padding = torch.zeros(teacher_logits.shape[0], target_len - current_len, teacher_logits.shape[2], device=teacher_logits.device)
                    teacher_logits = torch.cat([teacher_logits, padding], dim=1)
                elif current_len > target_len:
                     teacher_logits = teacher_logits[:, :target_len, :]

            else:
                 raise ValueError(f"Unknown input_key: {input_key}")


            return student_input, attention_mask, student_target, teacher_logits

        train_loader = DataLoader(train_wrapper, batch_size=batch_size, collate_fn=distillation_collate_fn, shuffle=True)
        val_loader = DataLoader(val_wrapper, batch_size=batch_size, collate_fn=distillation_collate_fn)

        return train_loader, val_loader


    async def run_distillation(
        self,
        train_loader: DataLoader,
        val_loader: DataLoader,
        epochs: int,
        model_id: str,
        task_description: str,
        student_config: Dict[str, Any],
    ) -> Dict[str, Any]:

        safe_model_id = model_id.lower().replace(" ", "_")
        print(f"--- Starting Knowledge Distillation for model: {safe_model_id} ---")

        final_metrics: Dict[str, float] = {}

        # 1. çŸ¥è­˜è’¸ç•™ã®å®Ÿè¡Œ
        print(f"Step 1: Running distillation training for {epochs} epochs...")
        for epoch in range(epochs):
            self.distillation_trainer.train_epoch(train_loader, epoch)
            # è©•ä¾¡ã¯æœ€çµ‚ã‚¨ãƒãƒƒã‚¯å¾Œã®ã¿å®Ÿæ–½ (é«˜é€ŸåŒ–ã®ãŸã‚)
            if epoch == epochs - 1:
                final_metrics = self.distillation_trainer.evaluate(val_loader, epoch)
        print("Distillation training finished.")

        # 2. ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ (æœ€çµ‚) - trainer.evaluateå†…ã§å®Ÿè¡Œæ¸ˆã¿ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ä½¿ç”¨
        print("Step 2: Evaluating the distilled model...")
        # evaluation_results = await self.evaluate_model(val_loader) # è©•ä¾¡ã¯train_epochå†…ã§å®Ÿæ–½æ¸ˆ
        # final_metrics.update(evaluation_results)
        print(f"Evaluation finished. Final Metrics: {final_metrics}")


        # 3. ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
        save_dir = os.path.join("runs", "specialists", safe_model_id)
        os.makedirs(save_dir, exist_ok=True)
        save_path = os.path.join(save_dir, "best_model.pth")
        print(f"Step 3: Saving the model to {save_path}...")

        model_to_save = self.distillation_trainer.model.module if isinstance(self.distillation_trainer.model, nn.parallel.DistributedDataParallel) else self.distillation_trainer.model
        # SNNCoreãƒ©ãƒƒãƒ‘ãƒ¼ã®å ´åˆã€ä¸­ã®ãƒ¢ãƒ‡ãƒ«ã‚’å–ã‚Šå‡ºã™
        if isinstance(model_to_save, SNNCore):
            model_to_save = model_to_save.model

        buffers_to_exclude = {
            name for name, _ in model_to_save.named_buffers()
            if any(keyword in name for keyword in ['mem', 'spikes', 'adaptive_threshold', 'pre_trace', 'post_trace', 'eligibility_trace', 'causal_contribution', 'v', 'u']) # SNNçŠ¶æ…‹å¤‰æ•°ã‚’é™¤å¤–
        }
        model_state_to_save = {k: v for k, v in model_to_save.state_dict().items() if k not in buffers_to_exclude}

        # ä¿å­˜ã™ã‚‹è¾æ›¸ã« student_config ã‚’å«ã‚ã‚‹
        save_dict = {
            'model_state_dict': model_state_to_save,
            'config': student_config # ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’ä¸€ç·’ã«ä¿å­˜
        }
        torch.save(save_dict, save_path)
        print("Model saved.")


        # 4. ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã¸ã®ç™»éŒ²
        print("Step 4: Registering the model...")
        await self.model_registry.register_model(
            model_id=safe_model_id,
            task_description=task_description,
            metrics=final_metrics,
            model_path=save_path, # ä¿å­˜ãƒ‘ã‚¹ã‚’æ¸¡ã™
            config=student_config
        )
        print(f"Model '{safe_model_id}' successfully registered.")

        print("--- Knowledge Distillation Finished ---")
        return {"model_id": safe_model_id, "metrics": final_metrics, "path": save_path, "config": student_config}

    async def run_on_demand_pipeline(self, task_description: str, unlabeled_data_path: str, force_retrain: bool, student_config: Optional[Dict[str, Any]] = None):
        """Webã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ç­‰ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿ã§ã‚ªãƒ³ãƒ‡ãƒãƒ³ãƒ‰å­¦ç¿’ã‚’å®Ÿè¡Œã™ã‚‹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€‚"""
        print(f"ğŸš€ Starting on-demand pipeline for task: {task_description}")

        if student_config is None:
            print("student_config not provided, attempting to retrieve from student model...")
            if hasattr(self.student_model, 'config') and isinstance(self.student_model, SNNCore): # SNNCoreã‹ç¢ºèª
                student_config_resolved = OmegaConf.to_container(self.student_model.config, resolve=True)
                student_config = cast(Dict[str, Any], student_config_resolved)
                print("âœ… Successfully retrieved config from SNNCore model.")
            else:
                raise ValueError("student_config was not provided and could not be retrieved from the model.")

        if student_config is None:
            raise ValueError("student_config is None, cannot proceed.")

        # --- â–¼ ä¿®æ­£: ç”»åƒã‚¿ã‚¹ã‚¯ã‹ãƒ†ã‚­ã‚¹ãƒˆã‚¿ã‚¹ã‚¯ã‹ã‚’åˆ¤å®š â–¼ ---
        # ã“ã“ã§ã¯ç°¡æ˜“çš„ã«ã€ã‚¿ã‚¹ã‚¯è¨˜è¿°ã«'image'ã‚„'cifar'ãŒå«ã¾ã‚Œã‚‹ã‹ã§åˆ¤å®š
        is_image_task = any(kw in task_description.lower() for kw in ['image', 'cifar', 'vision'])

        if is_image_task:
             # ç”»åƒã‚¿ã‚¹ã‚¯ç”¨ã®ãƒ‡ãƒ¼ã‚¿æº–å‚™ (ä¾‹: CIFAR10Taskã‚’ä½¿ç”¨)
             TaskClass = TASK_REGISTRY.get("cifar10") # ä»®ã«CIFAR10ã¨ã™ã‚‹
             if not TaskClass: raise ValueError("CIFAR10 task not found in registry.")
             task = TaskClass(tokenizer=self.tokenizer, device=self.device, hardware_profile={})
             train_dataset, val_dataset = task.prepare_data() # ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰
             collate_fn = task.get_collate_fn()
        else:
             # ãƒ†ã‚­ã‚¹ãƒˆã‚¿ã‚¹ã‚¯ç”¨ã®ãƒ‡ãƒ¼ã‚¿æº–å‚™
             from snn_research.data.datasets import SimpleTextDataset # ãƒ†ã‚­ã‚¹ãƒˆç”¨Datasetã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
             if not os.path.exists(unlabeled_data_path):
                 raise FileNotFoundError(f"Unlabeled data file not found: {unlabeled_data_path}")

             dataset = SimpleTextDataset(file_path=unlabeled_data_path, tokenizer=self.tokenizer, max_seq_len=student_config.get('time_steps', 128))
             # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ†å‰² (ä¾‹: 90% train, 10% val)
             train_size = int(0.9 * len(dataset))
             val_size = len(dataset) - train_size
             train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])

             from train import collate_fn as text_collate_fn # train.pyã‹ã‚‰collate_fnã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
             collate_fn = text_collate_fn(self.tokenizer, is_distillation=True) # è’¸ç•™ç”¨ã®collate_fnã‚’å–å¾—
        # --- â–² ä¿®æ­£ â–² ---

        # çŸ¥è­˜è’¸ç•™ç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ©ãƒƒãƒ—
        train_loader, val_loader = self.prepare_dataset(
            train_dataset=train_dataset,
            val_dataset=val_dataset,
            collate_fn=collate_fn, # é©åˆ‡ãªcollate_fnã‚’æ¸¡ã™
            batch_size=self.distillation_trainer.config.training.batch_size() if hasattr(self.distillation_trainer, 'config') else 8 # configãŒã‚ã‚Œã°ãã“ã‹ã‚‰ã€ãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        )

        # è’¸ç•™ã®å®Ÿè¡Œ
        result = await self.run_distillation(
            train_loader=train_loader,
            val_loader=val_loader,
            epochs=self.distillation_trainer.config.training.epochs() if hasattr(self.distillation_trainer, 'config') else 5, # configãŒã‚ã‚Œã°ãã“ã‹ã‚‰ã€ãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
            model_id=task_description,
            task_description=f"Expert for {task_description}",
            student_config=student_config
        )
        return result


    async def evaluate_model(self, dataloader: DataLoader) -> Dict[str, float]:
        """
        è’¸ç•™æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’è©•ä¾¡ã™ã‚‹ã€‚
        """
        model_to_eval = self.distillation_trainer.model
        model_to_eval.eval()
        total_spikes = 0.0
        total_samples = 0
        num_neurons = 0

        # SNNCoreãƒ©ãƒƒãƒ‘ãƒ¼ã®å ´åˆã€ä¸­ã®å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’è¨ˆç®—
        if isinstance(model_to_eval, SNNCore):
            num_neurons = sum(p.numel() for p in model_to_eval.model.parameters() if p.requires_grad)
        else:
            num_neurons = sum(p.numel() for p in model_to_eval.parameters() if p.requires_grad)

        progress_bar = tqdm(dataloader, desc="Evaluating Distilled Model")
        all_logits = []
        all_labels = []

        for batch in progress_bar:
            # distillation_collate_fn ã‹ã‚‰ã®ã‚¿ãƒ—ãƒ«ã‚’ã‚¢ãƒ³ãƒ‘ãƒƒã‚¯
            inputs, attention_mask, labels, teacher_logits = batch
            inputs = inputs.to(self.device)
            labels = labels.to(self.device)

            with torch.no_grad():
                # --- â–¼ ä¿®æ­£: å…¥åŠ›ã‚­ãƒ¼ã‚’åˆ¤å®š â–¼ ---
                input_key = "input_images" if inputs.ndim == 4 else "input_ids"
                model_input = {input_key: inputs}
                if input_key == "input_ids":
                    model_input["attention_mask"] = attention_mask.to(self.device)
                # --- â–² ä¿®æ­£ â–² ---

                outputs = model_to_eval(**model_input, return_spikes=True) # **ã§è¾æ›¸ã‚’å±•é–‹ã—ã¦æ¸¡ã™
                if isinstance(outputs, tuple) and len(outputs) > 1:
                    logits, avg_batch_spikes, _ = outputs
                else:
                    logits = outputs # ç”»åƒåˆ†é¡ãªã©ã¯ã‚¿ãƒ—ãƒ«ã§ãªã„å ´åˆãŒã‚ã‚‹
                    avg_batch_spikes = torch.zeros((), device=inputs.device)

            total_spikes += avg_batch_spikes.item() * inputs.size(0)
            total_samples += inputs.size(0)
            all_logits.append(logits.cpu())
            all_labels.append(labels.cpu())

        avg_spikes_per_sample = total_spikes / total_samples if total_samples > 0 else 0.0
        energy = calculate_energy_consumption(avg_spikes_per_sample, num_neurons=num_neurons)

        # Accuracyãªã©ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¯Trainerã®evaluateãƒ¡ã‚½ãƒƒãƒ‰ã§è¨ˆç®—ã•ã‚Œã‚‹ãŸã‚ã€ã“ã“ã§ã¯ã‚¹ãƒ‘ã‚¤ã‚¯ã¨ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®ã¿è¿”ã™
        # Note: ã‚‚ã—Trainerã®evaluateãŒå‘¼ã°ã‚Œãªã„å ´åˆã¯ã€ã“ã“ã§Accuracyè¨ˆç®—ã‚’è¿½åŠ ã™ã‚‹å¿…è¦ãŒã‚ã‚‹
        metrics = {
            "avg_spikes_per_sample": avg_spikes_per_sample,
            "estimated_energy_consumption": energy
        }
        # ä»–ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆä¾‹ï¼šaccuracyï¼‰ã¯ trainer.evaluate ã®çµæœã‹ã‚‰å–å¾—ã•ã‚Œã‚‹

        return metrics
