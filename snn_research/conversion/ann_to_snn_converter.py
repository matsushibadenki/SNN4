# matsushibadenki/snn_research/conversion/ann_to_snn_converter.py
# (æ›´æ–°)
# GGUF/Safetensorså½¢å¼ã®ANNãƒ¢ãƒ‡ãƒ«ã‹ã‚‰SNNã¸ã®å¤‰æ›ãƒ»è’¸ç•™ã‚’è¡Œã†ã‚³ãƒ³ãƒãƒ¼ã‚¿
#
# æ©Ÿèƒ½:
# - æŒ‡å®šã•ã‚ŒãŸãƒ‘ã‚¹ã‹ã‚‰Safetensorsã¾ãŸã¯GGUFãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã€‚
# - ANN-SNNå¤‰æ›: ANNã®é‡ã¿ã‚’SNNãƒ¢ãƒ‡ãƒ«ã«ç›´æ¥ã‚³ãƒ”ãƒ¼ã™ã‚‹ã€‚
# - ã‚ªãƒ³ãƒ©ã‚¤ãƒ³çŸ¥è­˜è’¸ç•™: ANNã‚’æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã€SNNã‚’å­¦ç¿’ã•ã›ã‚‹ã€‚
# - é–¾å€¤ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ©Ÿèƒ½ã‚’è¿½åŠ ã—ã€å¤‰æ›å¾Œã®SNNã®æ´»å‹•ã‚’å®‰å®šã•ã›ã‚‹ã€‚
# - [æ”¹å–„] GGUFãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿æ©Ÿèƒ½ã‚’æ­£å¼ã«å®Ÿè£…ã€‚
# - [æ”¹å–„ v2] LLMå¤‰æ›ç”¨ã®é«˜å¿ å®Ÿåº¦å¤‰æ›ãƒ¡ã‚½ãƒƒãƒ‰ `convert_llm_weights` ã‚’è¿½åŠ ã€‚

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from safetensors.torch import load_file
from tqdm import tqdm  # type: ignore
from typing import Dict, Any, Optional, Iterator
from gguf import GGUFReader
from transformers import AutoModelForCausalLM

from snn_research.benchmark.ann_baseline import ANNBaselineModel
from snn_research.core.snn_core import AdaptiveLIFNeuron, BreakthroughSNN
from .conversion_utils import normalize_weights

def _load_gguf(path: str) -> Dict[str, torch.Tensor]:
    """GGUFãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€PyTorchã®state_dictã‚’è¿”ã™ã€‚"""
    print(f" GGUFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {path}")
    reader = GGUFReader(path, 'r')
    state_dict = {}
    for tensor in reader.tensors:
        state_dict[tensor.name] = torch.from_numpy(tensor.data.copy())
    print(f"âœ… GGUFã‹ã‚‰ {len(state_dict)} å€‹ã®ãƒ†ãƒ³ã‚½ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸã€‚")
    return state_dict

class AnnToSnnConverter:
    """
    æ—¢å­˜ã®ANNãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰SNNãƒ¢ãƒ‡ãƒ«ã‚’ç”Ÿæˆã™ã‚‹ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã€‚
    """
    def __init__(self, snn_model: nn.Module, model_config: Dict[str, Any]):
        self.snn_model = snn_model
        self.model_config = model_config
        self.device = "cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu"
        self.snn_model.to(self.device)

    def _load_ann_weights(self, ann_model_path: str) -> Dict[str, torch.Tensor]:
        """ANNãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚€ã€‚"""
        print(f"ğŸ’¾ ANNãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {ann_model_path}")
        if ann_model_path.endswith(".safetensors"):
            return load_file(ann_model_path, device=self.device)
        elif ann_model_path.endswith(".gguf"):
            return _load_gguf(ann_model_path)
        else:
            # Hugging Faceã®ãƒ¢ãƒ‡ãƒ«IDã¾ãŸã¯ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã‚’æƒ³å®š
            try:
                model = AutoModelForCausalLM.from_pretrained(ann_model_path)
                return model.state_dict()
            except Exception as e:
                raise ValueError(f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã¾ãŸã¯ãƒ¢ãƒ‡ãƒ«IDã§ã™: {ann_model_path}. Error: {e}")

    def calibrate_thresholds(self, calibration_loader: Any, target_rate: float = 0.1, epochs: int = 1):
        """
        å¤‰æ›å¾Œã®SNNãƒ¢ãƒ‡ãƒ«ã®ç™ºç«é–¾å€¤ã‚’ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã™ã‚‹ã€‚
        """
        print(f"âš™ï¸ ç™ºç«é–¾å€¤ã®ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é–‹å§‹ã—ã¾ã™ (ç›®æ¨™ç™ºç«ç‡: {target_rate:.2f})...")
        self.snn_model.train()

        lif_layers = [m for m in self.snn_model.modules() if isinstance(m, AdaptiveLIFNeuron)]
        if not lif_layers:
            print("âš ï¸ é©å¿œçš„é–¾å€¤ã‚’æŒã¤LIFãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            return

        for layer in lif_layers:
            layer.target_spike_rate = target_rate

        with torch.no_grad():
            for epoch in range(epochs):
                for batch in tqdm(calibration_loader, desc=f"Calibration Epoch {epoch+1}"):
                    inputs = batch[0].to(self.device) if isinstance(batch, (list, tuple)) else batch.to(self.device)
                    self.snn_model(inputs)

        print("âœ… ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
        self.snn_model.eval()

    def convert_weights(
        self,
        ann_model_path: str,
        output_path: str,
        calibration_loader: Optional[Any] = None
    ) -> None:
        """
        ANN-SNNå¤‰æ›ï¼ˆé‡ã¿ã‚³ãƒ”ãƒ¼ï¼‰ã‚’å®Ÿè¡Œã—ã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§é–¾å€¤ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œã†ã€‚
        """
        ann_weights = self._load_ann_weights(ann_model_path)
        snn_state_dict = self.snn_model.state_dict()

        print("ğŸ”„ ANNã®é‡ã¿ã‚’SNNãƒ¢ãƒ‡ãƒ«ã«ã‚³ãƒ”ãƒ¼ã—ã¦ã„ã¾ã™...")
        
        copied_keys = 0
        for name, param in snn_state_dict.items():
            if name in ann_weights and param.shape == ann_weights[name].shape:
                snn_state_dict[name].copy_(ann_weights[name])
                copied_keys += 1
        
        print(f"  - {copied_keys}å€‹ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸã€‚")
        self.snn_model.load_state_dict(snn_state_dict, strict=False)

        if calibration_loader:
            self.calibrate_thresholds(calibration_loader)
        
        torch.save({
            'model_state_dict': self.snn_model.state_dict(),
            'config': self.model_config
        }, output_path)
        print(f"âœ… é‡ã¿å¤‰æ›ãŒå®Œäº†ã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’ '{output_path}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

    def convert_llm_weights(
        self,
        ann_model_name_or_path: str,
        output_path: str,
        calibration_loader: Optional[Any] = None
    ) -> None:
        """
        Hugging Faceã®LLMã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€æ­£è¦åŒ–ã¨é«˜åº¦ãªãƒãƒƒãƒ”ãƒ³ã‚°ã‚’è¡Œã£ã¦SNNã«å¤‰æ›ã™ã‚‹ã€‚
        """
        print(f"--- ğŸš€ é«˜å¿ å®Ÿåº¦LLMå¤‰æ›é–‹å§‹: {ann_model_name_or_path} ---")
        
        # 1. ANNãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
        ann_model = AutoModelForCausalLM.from_pretrained(ann_model_name_or_path).to(self.device)
        ann_model.eval()

        # 2. é‡ã¿æ­£è¦åŒ–
        normalized_ann_weights = normalize_weights(ann_model)

        # 3. é«˜åº¦ãªé‡ã¿ãƒãƒƒãƒ”ãƒ³ã‚°
        snn_state_dict = self.snn_model.state_dict()
        print("ğŸ”„ é«˜åº¦ãªé‡ã¿ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œä¸­...")
        copied_count = 0
        missed_count = 0

        #
        # ã“ã“ã«ã€ANN (ä¾‹: GPT2) ã¨ SNN (ä¾‹: SpikingTransformer) ã®
        # ãƒ¬ã‚¤ãƒ¤ãƒ¼æ§‹é€ ã®é•ã„ã‚’å¸åã™ã‚‹ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè£…ã—ã¾ã™ã€‚
        # ã“ã‚Œã¯éå¸¸ã«è¤‡é›‘ã§ã€ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«å¼·ãä¾å­˜ã—ã¾ã™ã€‚
        #
        # ä¾‹: GPT2ã® 'transformer.h.{i}.attn.c_attn' ã¯SNNã§ã¯ 'q_proj', 'k_proj', 'v_proj' ã«åˆ†é›¢ã•ã‚Œã¦ã„ã‚‹
        #
        for ann_name, ann_param in normalized_ann_weights.items():
            # ã“ã“ã§ã¯å˜ç´”ãªåå‰ãƒ™ãƒ¼ã‚¹ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’è©¦ã¿ã‚‹ãŒã€æœ¬æ¥ã¯æ­£è¦è¡¨ç¾ã‚„æ§‹é€ è§£æãŒå¿…è¦
            if ann_name in snn_state_dict and snn_state_dict[ann_name].shape == ann_param.shape:
                snn_state_dict[ann_name].copy_(ann_param)
                copied_count += 1
            else:
                missed_count += 1
        
        print(f"  - {copied_count}å€‹ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç›´æ¥ãƒãƒƒãƒ”ãƒ³ã‚°ã—ã¾ã—ãŸã€‚")
        print(f"  - {missed_count}å€‹ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ãƒãƒƒãƒ”ãƒ³ã‚°ã§ãã¾ã›ã‚“ã§ã—ãŸï¼ˆè¦èª¿æŸ»ï¼‰ã€‚")

        self.snn_model.load_state_dict(snn_state_dict, strict=False)

        # 4. é–¾å€¤ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        if calibration_loader:
            self.calibrate_thresholds(calibration_loader)
        else:
            print("âš ï¸ ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ãŒæä¾›ã•ã‚Œãªã‹ã£ãŸãŸã‚ã€é–¾å€¤èª¿æ•´ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")

        # 5. å¤‰æ›æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
        torch.save({
            'model_state_dict': self.snn_model.state_dict(),
            'config': self.model_config
        }, output_path)
        print(f"âœ… LLMå¤‰æ›ãŒå®Œäº†ã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’ '{output_path}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")


    def run_online_distillation(
        self,
        ann_teacher_model: nn.Module,
        dummy_data_loader: Any, # æœ¬æ¥ã¯å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
        output_path: str,
        epochs: int = 3
    ) -> None:
        """
        ã‚ªãƒ³ãƒ©ã‚¤ãƒ³çŸ¥è­˜è’¸ç•™ã‚’å®Ÿè¡Œã™ã‚‹ã€‚
        """
        ann_teacher_model.to(self.device)
        ann_teacher_model.eval()

        optimizer = optim.AdamW(self.snn_model.parameters(), lr=1e-4)
        loss_fn = nn.KLDivLoss(reduction='batchmean', log_target=True)

        print("ğŸ”¥ ã‚ªãƒ³ãƒ©ã‚¤ãƒ³çŸ¥è­˜è’¸ç•™ã‚’é–‹å§‹ã—ã¾ã™...")
        self.snn_model.train()

        for epoch in range(epochs):
            progress_bar = tqdm(dummy_data_loader, desc=f"Distillation Epoch {epoch+1}")
            for batch in progress_bar:
                inputs = batch[0].to(self.device) if isinstance(batch, (list, tuple)) else batch.to(self.device)
                
                optimizer.zero_grad()
                
                snn_logits, _, _ = self.snn_model(inputs)
                
                with torch.no_grad():
                    teacher_outputs = ann_teacher_model(inputs)
                    teacher_logits = teacher_outputs.logits if hasattr(teacher_outputs, 'logits') else teacher_outputs
                
                loss = loss_fn(
                    F.log_softmax(snn_logits / 2.0, dim=-1),
                    F.log_softmax(teacher_logits / 2.0, dim=-1)
                )
                
                loss.backward()
                optimizer.step()
                progress_bar.set_postfix({"loss": loss.item()})
        
        torch.save({
            'model_state_dict': self.snn_model.state_dict(),
            'config': self.model_config
        }, output_path)
        print(f"âœ… çŸ¥è­˜è’¸ç•™ãŒå®Œäº†ã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’ '{output_path}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")