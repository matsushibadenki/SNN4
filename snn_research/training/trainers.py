# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/training/trainers.py
# (çœç•¥...)
# ä¿®æ­£ç‚¹ (v8): ParticleFilterTrainerã®ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ã‚’ä¿®æ­£ã—ã€
#           KeyError: 'training' ã‚’è§£æ¶ˆã™ã‚‹ã€‚

# (...ãƒ•ã‚¡ã‚¤ãƒ«ã®å…ˆé ­ã¯çœç•¥...)

class BPTTTrainer:
    # (...ã“ã®ã‚¯ãƒ©ã‚¹ã¯å¤‰æ›´ãªã—...)
    pass

class ParticleFilterTrainer:
    """
    é€æ¬¡ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ï¼ˆãƒ‘ãƒ¼ãƒ†ã‚£ã‚¯ãƒ«ãƒ•ã‚£ãƒ«ã‚¿ï¼‰ã‚’ç”¨ã„ã¦ã€å¾®åˆ†ä¸å¯èƒ½ãªSNNã‚’å­¦ç¿’ã™ã‚‹ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã€‚
    CPUä¸Šã§ã®å®Ÿè¡Œã‚’æƒ³å®šã—ã€GPUä¾å­˜ã‹ã‚‰è„±å´ã™ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã€‚
    """
    def __init__(self, base_model: BioSNN, config: Dict[str, Any], device: str):
        self.base_model = base_model.to(device)
        self.device = device
        self.config = config
        # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†“ä¿®æ­£é–‹å§‹â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
        # configã®éšŽå±¤æ§‹é€ ã‚’å¤‰æ›´
        self.num_particles = config['num_particles']
        self.noise_std = config['noise_std']
        # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†‘ä¿®æ­£çµ‚ã‚ã‚Šâ—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
        
        self.particles = [copy.deepcopy(self.base_model) for _ in range(self.num_particles)]
        self.particle_weights = torch.ones(self.num_particles, device=self.device) / self.num_particles
        print(f"ðŸŒªï¸ ParticleFilterTrainer initialized with {self.num_particles} particles.")

    def train_step(self, data: torch.Tensor, targets: torch.Tensor) -> float:
        """1ã‚¹ãƒ†ãƒƒãƒ—ã®å­¦ç¿’ï¼ˆäºˆæ¸¬ã€å°¤åº¦è¨ˆç®—ã€å†ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰ã‚’å®Ÿè¡Œã™ã‚‹ã€‚"""
        data = data.to(self.device)
        targets = targets.to(self.device)

        for particle in self.particles:
            with torch.no_grad():
                for param in particle.parameters():
                    param.add_(torch.randn_like(param) * self.noise_std)
        
        log_likelihoods = []
        for particle in self.particles:
            particle.eval()
            with torch.no_grad():
                squeezed_data = data.squeeze(0) if data.dim() > 1 else data
                input_spikes = (torch.rand_like(squeezed_data) > 0.5).float().to(self.device)
                outputs, _ = particle(input_spikes)
                
                squeezed_targets = targets.squeeze(0) if targets.dim() > 1 else targets
                loss = F.mse_loss(outputs, squeezed_targets)
                log_likelihoods.append(-loss)
        
        log_likelihoods_tensor = torch.tensor(log_likelihoods, device=self.device)
        self.particle_weights *= torch.exp(log_likelihoods_tensor - log_likelihoods_tensor.max())
        
        if self.particle_weights.sum() > 0:
            self.particle_weights /= self.particle_weights.sum()
        else:
            self.particle_weights.fill_(1.0 / self.num_particles)

        if 1. / (self.particle_weights**2).sum() < self.num_particles / 2:
            indices = torch.multinomial(self.particle_weights, self.num_particles, replacement=True)
            new_particles = [copy.deepcopy(self.particles[i]) for i in indices]
            self.particles = new_particles
            self.particle_weights.fill_(1.0 / self.num_particles)
        
        best_particle_loss = -log_likelihoods_tensor.max().item()
        return best_particle_loss
