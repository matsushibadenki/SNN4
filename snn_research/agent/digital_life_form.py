# „Éï„Ç°„Ç§„É´„Éë„Çπ: snn_research/agent/digital_life_form.py
# (Êõ¥Êñ∞)
# ÊîπÂñÑÁÇπ:
# - DigitalLifeForm„ÅåHierarchicalPlanner„ÇíÁõ¥Êé•Âà©Áî®„Åó„Å¶„ÄÅ
#   È´ò„É¨„Éô„É´„ÅÆÁõÆÊ®ô„Åã„ÇâÂÖ∑‰ΩìÁöÑ„Å™Ë°åÂãïË®àÁîª„ÇíÁ´ãÊ°à„ÉªÂÆüË°å„Åô„Çã„Çà„ÅÜ„Å´‰øÆÊ≠£„ÄÇ
# - `_decide_next_action`„Çí`_formulate_goal`„Å´ÊîπÂêç„Åó„ÄÅËá™ÁÑ∂Ë®ÄË™û„ÅÆÁõÆÊ®ô„ÇíÁîüÊàê„Åô„Çã„Çà„ÅÜ„Å´„Åó„Åü„ÄÇ
# - `life_cycle_step`„Åß„ÄÅÁîüÊàê„Åï„Çå„ÅüË®àÁîª„Çí„É´„Éº„ÉóÂÆüË°å„Åô„Çã„É≠„Ç∏„ÉÉ„ÇØ„ÇíÂÆüË£Ö„ÄÇ

import time
import logging
import torch
import random
import json
import asyncio
from typing import Dict, Any, Optional, List, TYPE_CHECKING
import operator
import os

from snn_research.cognitive_architecture.intrinsic_motivation import IntrinsicMotivationSystem
from snn_research.cognitive_architecture.meta_cognitive_snn import MetaCognitiveSNN
from snn_research.agent.memory import Memory
from snn_research.cognitive_architecture.physics_evaluator import PhysicsEvaluator
from snn_research.cognitive_architecture.symbol_grounding import SymbolGrounding
from snn_research.agent.autonomous_agent import AutonomousAgent
from snn_research.agent.reinforcement_learner_agent import ReinforcementLearnerAgent
from snn_research.agent.self_evolving_agent import SelfEvolvingAgent
from snn_research.cognitive_architecture.global_workspace import GlobalWorkspace
# HierarchicalPlanner„Çí„Ç§„É≥„Éù„Éº„Éà
from snn_research.cognitive_architecture.hierarchical_planner import HierarchicalPlanner

if TYPE_CHECKING:
    from app.adapters.snn_langchain_adapter import SNNLangChainAdapter
    from snn_research.training.bio_trainer import BioRLTrainer
    from snn_research.rl_env.grid_world import GridWorldEnv

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class DigitalLifeForm:
    """
    „Éó„É©„É≥„Éä„Éº„Å®ÈÄ£Êê∫„Åó„ÄÅÁõÆÊ®ô„Å´Âü∫„Å•„ÅÑ„ÅüË®àÁîª„ÇíÂÆüË°å„Åô„Çã„ÄÅÈÄ≤Âåñ„Åó„Åü„Ç™„Éº„Ç±„Çπ„Éà„É¨„Éº„Çø„Éº„ÄÇ
    """
    def __init__(
        self,
        planner: HierarchicalPlanner,
        autonomous_agent: AutonomousAgent,
        rl_agent: ReinforcementLearnerAgent,
        self_evolving_agent: SelfEvolvingAgent,
        motivation_system: IntrinsicMotivationSystem,
        meta_cognitive_snn: MetaCognitiveSNN,
        memory: Memory,
        physics_evaluator: PhysicsEvaluator,
        symbol_grounding: SymbolGrounding,
        langchain_adapter: "SNNLangChainAdapter",
        global_workspace: GlobalWorkspace
    ):
        # --- ‚ñº ‰øÆÊ≠£ ‚ñº ---
        self.planner = planner
        # --- ‚ñ≤ ‰øÆÊ≠£ ‚ñ≤ ---
        self.autonomous_agent = autonomous_agent
        self.rl_agent = rl_agent
        self.self_evolving_agent = self_evolving_agent
        self.motivation_system = motivation_system
        self.meta_cognitive_snn = meta_cognitive_snn
        self.memory = memory
        self.physics_evaluator = physics_evaluator
        self.symbol_grounding = symbol_grounding
        self.langchain_adapter = langchain_adapter
        self.workspace = global_workspace
        
        self.running = False
        self.state: Dict[str, Any] = {"last_action": None, "last_result": None, "last_task": "unknown"}

    # ... (start, stop, life_cycle„É°„ÇΩ„ÉÉ„Éâ„ÅØÂ§âÊõ¥„Å™„Åó) ...
    def start(self): self.running = True; logging.info("DigitalLifeForm activated."); self.life_cycle()
    def stop(self): self.running = False; logging.info("DigitalLifeForm deactivating.")
    def life_cycle(self):
        while self.running: self.life_cycle_step(); time.sleep(10)

    # --- ‚ñº ‰øÆÊ≠£ ‚ñº ---
    def life_cycle_step(self):
        """Ë®àÁîª‰∏ªÂ∞é„ÅÆË™çÁü•„Çµ„Ç§„ÇØ„É´„ÇíÂÆüË°å„Åô„Çã„ÄÇ"""
        logging.info("\n--- üß† New Cognitive Cycle ---")
        self._handle_causal_credit()

        # 1. ÂÜÖÈÉ®Áä∂ÊÖã„ÇíË©ï‰æ°„Åó„ÄÅÈ´ò„É¨„Éô„É´„ÅÆÁõÆÊ®ô„ÇíÁ≠ñÂÆö„Åô„Çã
        internal_state = self.motivation_system.get_internal_state()
        performance_eval = self.meta_cognitive_snn.evaluate_performance()
        goal = self._formulate_goal(internal_state, performance_eval)
        logging.info(f"üéØ New Goal: {goal}")
        
        # 2. „Éó„É©„É≥„Éä„Éº„Å´ÁõÆÊ®ô„ÇíÊ∏°„Åó„ÄÅË°åÂãïË®àÁîª„ÇíÁ´ãÊ°à„Åï„Åõ„Çã
        plan = asyncio.run(self.planner.create_plan(goal))

        # 3. Ë®àÁîª„Åï„Çå„Åü„Çø„Çπ„ÇØ„ÇíÈ†ÜÁï™„Å´ÂÆüË°å„Åô„Çã
        if not plan.task_list:
            logging.warning("Planner could not create a plan. Idling for this cycle.")
            return

        logging.info(f"üìã Plan Created: {[task.get('task') for task in plan.task_list]}")
        for task in plan.task_list:
            action = task.get('task')
            if not action: continue

            logging.info(f"‚ñ∂Ô∏è Executing task from plan: {action}")
            result, reward, expert_used = self._execute_action(action, internal_state)

            # 4. ÂêÑ„Çπ„ÉÜ„ÉÉ„Éó„ÅÆÁµêÊûú„ÇíË®òÈå≤„ÉªË©ï‰æ°„Åô„Çã
            if isinstance(result, dict): self.symbol_grounding.process_observation(result, context=f"action '{action}'")
            reward_vector = {"external": reward, "curiosity": internal_state.get("curiosity", 0.0)}
            decision_context = {"goal": goal, "plan": [t.get('task') for t in plan.task_list]}
            self.memory.record_experience(self.state, action, result, reward, expert_used, decision_context)
            
            # (Á∞°ÊòìÁöÑ„Å™ÂãïÊ©üÊõ¥Êñ∞)
            self.motivation_system.update_metrics(random.random(), 1.0 if reward > 0 else 0.0, random.random(), random.random())

            self.state["last_action"] = action; self.state["last_result"] = result
            logging.info(f"  - Task Result: {str(result)[:100]}, Reward: {reward:.2f}")

            if reward < 0:
                logging.warning(f"  - Task '{action}' failed. Aborting current plan.")
                break # Ë®àÁîª„ÅÆÈÄî‰∏≠„ÅßÂ§±Êïó„Åó„Åü„Çâ‰∏≠Ê≠¢

    def _formulate_goal(self, internal_state: Dict[str, Any], performance_eval: Dict[str, Any]) -> str:
        """ÂÜÖÈÉ®Áä∂ÊÖã„Å®„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπË©ï‰æ°„Åã„Çâ„ÄÅËá™ÁÑ∂Ë®ÄË™û„ÅÆÁõÆÊ®ô„ÇíÁîüÊàê„Åô„Çã„ÄÇ"""
        if internal_state.get("curiosity", 0.0) > 0.8 and internal_state.get("curiosity_context"):
            topic = internal_state.get("curiosity_context")
            return f"Explore the unknown concept related to '{str(topic)}' to satisfy curiosity."
        
        if performance_eval.get("status") == "capability_gap":
            return "Evolve my architecture to overcome a capability gap."

        if internal_state.get("boredom", 0.0) > 0.7:
            return "Explore a completely new and random task to alleviate boredom."

        return "Practice an existing skill to improve confidence and performance."

    # --- ‚ñº ‰øÆÊ≠£ ‚ñº ---
    def _handle_causal_credit(self):
        """GlobalWorkspace„ÇíÁõ£Ë¶ñ„Åó„ÄÅÂõ†ÊûúÁöÑ„ÇØ„É¨„Ç∏„ÉÉ„Éà‰ø°Âè∑„Åå„ÅÇ„Çå„Å∞Âá¶ÁêÜ„Åô„Çã„ÄÇ"""
        # Workspace„Åã„ÇâÊÑèË≠ò„Å´‰∏ä„Å£„ÅüÊúÄÊñ∞„ÅÆÊÉÖÂ†±„ÇíÂèñÂæó
        conscious_content = self.workspace.conscious_broadcast_content
        
        if conscious_content and isinstance(conscious_content, dict) and conscious_content.get("type") == "causal_credit":
            target_action = conscious_content.get("target_action")
            credit = conscious_content.get("credit", 0.0)
            
            print(f"‚ú® Âõ†ÊûúÁöÑ„ÇØ„É¨„Ç∏„ÉÉ„Éà‰ø°Âè∑„ÇíÊ§úÁü•ÔºÅ Target: {target_action}, Credit: {credit}")

            # Áõ¥Ëøë„ÅÆË°åÂãï„Åå„ÇØ„É¨„Ç∏„ÉÉ„Éà„ÅÆÂØæË±°„Å®‰∏ÄËá¥„Åô„Çã„ÅãÁ¢∫Ë™ç
            if self.state.get("last_action") and target_action == f"action_{self.state['last_action']}":
                print(f"  - Áõ¥Ëøë„ÅÆË°åÂãï '{self.state['last_action']}' „Å´„ÇØ„É¨„Ç∏„ÉÉ„Éà„ÇíÂâ≤„ÇäÂΩì„Å¶„Åæ„Åô„ÄÇ")
                
                # „Åì„Åì„Åß„ÄÅ„ÇØ„É¨„Ç∏„ÉÉ„Éà„ÇíÈÅ©Âàá„Å™„Ç®„Éº„Ç∏„Çß„É≥„Éà„ÅÆÂ≠¶ÁøíÂâá„Å´Ê∏°„Åô
                # „Åì„ÅÆ‰æã„Åß„ÅØ„ÄÅRL„Ç®„Éº„Ç∏„Çß„É≥„Éà„ÅåÂØæË±°„Å†„Å®‰ªÆÂÆö
                if self.state['last_action'] in ["explore_new_task_with_rl", "practice_skill_with_rl"]:
                    # learn„É°„ÇΩ„ÉÉ„Éâ„Å´ÁâπÂà•„Å™„Éë„É©„É°„Éº„Çø„ÇíÊ∏°„Åô
                    self.rl_agent.learn(reward=0.0, causal_credit=credit)
                    print("  - RL„Ç®„Éº„Ç∏„Çß„É≥„Éà„ÅÆ„Ç∑„Éä„Éó„ÇπÂèØÂ°ëÊÄß„Çí„Éà„ÉÉ„Éó„ÉÄ„Ç¶„É≥„ÅßÂ§âË™ø„Åó„Åæ„Åó„Åü„ÄÇ")

    def _decide_next_action(self, internal_state: Dict[str, Any], performance_eval: Dict[str, Any]) -> str:
        # (Â§âÊõ¥„Å™„Åó)
        action_scores: Dict[str, float] = { "explore_curiosity": internal_state.get("curiosity", 0.0) * 20.0, "evolve_architecture": 0.0, "practice_skill_with_rl": internal_state.get("confidence", 0.5) * 2.0 }
        if performance_eval.get("status") == "capability_gap": action_scores["evolve_architecture"] += 10.0
        if internal_state.get("boredom", 0.0) > 0.8: action_scores["explore_curiosity"] += internal_state.get("boredom", 0.0) * 15.0
        chosen_action = max(action_scores.items(), key=operator.itemgetter(1))[0]
        logging.info(f"Action scores: {action_scores} -> Chosen: {chosen_action}")
        return chosen_action
    # --- ‚ñ≤ ‰øÆÊ≠£ ‚ñ≤ ---

    def _execute_action(self, action: str, internal_state: Dict[str, Any]) -> tuple[Dict[str, Any], float, List[str]]:
        # (Â§âÊõ¥„Å™„Åó)
        from snn_research.rl_env.grid_world import GridWorldEnv
        from snn_research.training.bio_trainer import BioRLTrainer
        try:
            if action == "explore_curiosity":
                curiosity_topic = internal_state.get("curiosity_context")
                if not curiosity_topic: return {"status": "skipped", "info": "No specific curiosity context found."}, 0.0, []
                topic_str = str(curiosity_topic.get("action", "AI concept"))
                logging.info(f"üî¨ Curiosity triggered! Researching topic: '{topic_str}'")
                new_model_info = asyncio.run(self.autonomous_agent.handle_task(task_description=topic_str, unlabeled_data_path="data/sample_data.jsonl", force_retrain=True))
                if new_model_info: return {"status": "success", "info": f"Learned about '{topic_str}' and created new expert.", "model_info": new_model_info}, 1.0, ["autonomous_agent"]
                else: return {"status": "failure", "info": f"Failed to learn about '{topic_str}'."}, -0.5, ["autonomous_agent"]
            elif action == "evolve_architecture":
                return {"status": "success", "info": self.self_evolving_agent.evolve()}, 0.9, ["self_evolver"]
            elif action == "practice_skill_with_rl":
                env = GridWorldEnv(size=5, max_steps=20, device=self.rl_agent.device)
                trainer = BioRLTrainer(agent=self.rl_agent, env=env)
                res = trainer.train(num_episodes=10)
                return {"status": "success", "results": res}, res.get("final_average_reward", 0.0), ["rl_agent"]
            else: return {"status": "idle", "info": "No compelling action to take."}, 0.0, []
        except Exception as e:
            logging.error(f"Error executing action '{action}': {e}")
            return {"status": "error", "info": str(e)}, -1.0, []

    def awareness_loop(self, cycles: int):
        # (Â§âÊõ¥„Å™„Åó)
        print(f"üß¨ Digital Life Form awareness loop starting for {cycles} cycles.")
        self.running = True
        for i in range(cycles):
            if not self.running: break
            print(f"\n----- Cycle {i+1}/{cycles} -----")
            self.life_cycle_step()
            time.sleep(2)
        print("üß¨ Awareness loop finished.")

    def explain_last_action(self) -> Optional[str]:
        try:
            with open(self.memory.memory_path, "rb") as f:
                try:
                    f.seek(-2, os.SEEK_END)
                    while f.read(1) != b'\n': f.seek(-2, os.SEEK_CUR)
                except OSError: f.seek(0)
                last_line = f.readline().decode()
            last_experience = json.loads(last_line)
        except (IOError, json.JSONDecodeError, IndexError):
            return "Ë°åÂãïÂ±•Ê≠¥„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì„ÄÇ"

        prompt = f"""
        „ÅÇ„Å™„Åü„ÅØ„ÄÅËá™Ë∫´„ÅÆË°åÂãï„ÇíÂàÜÊûê„Åó„ÄÅ„Åù„ÅÆÁêÜÁî±„ÇíÂàÜ„Åã„Çä„ÇÑ„Åô„ÅèË™¨Êòé„Åô„ÇãAI„Åß„Åô„ÄÇ
        ‰ª•‰∏ã„ÅÆÂÜÖÈÉ®„É≠„Ç∞„ÅØ„ÄÅ„ÅÇ„Å™„ÅüËá™Ë∫´„ÅÆÁõ¥Ëøë„ÅÆË°åÂãïË®òÈå≤„Åß„Åô„ÄÇ„Åì„ÅÆË®òÈå≤„ÇíÂü∫„Å´„ÄÅ„Å™„Åú„Åù„ÅÆË°åÂãï„ÇíÂèñ„Å£„Åü„ÅÆ„Åã„Çí‰∏Ä‰∫∫Áß∞Ôºà„ÄåÁßÅ„ÄçÔºâ„ÅßË™¨Êòé„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

        ### Ë°åÂãï„É≠„Ç∞
        - **ÂÆüË°å„Åó„ÅüË°åÂãï:** {last_experience.get('action')}
        - **ÊÑèÊÄùÊ±∫ÂÆö„ÅÆÊ†πÊã†:**
          - **ÂÜÖÁô∫ÁöÑÂãïÊ©üÔºàÂÜÖÈÉ®Áä∂ÊÖãÔºâ:** {last_experience.get('decision_context', {}).get('internal_state')}
          - **Ëá™Â∑±„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπË©ï‰æ°:** {last_experience.get('decision_context', {}).get('performance_eval')}
          - **Áâ©ÁêÜÂäπÁéáË©ï‰æ°:** {last_experience.get('decision_context', {}).get('physical_rewards')}

        ### ÊåáÁ§∫
        ‰∏äË®ò„ÅÆÊ†πÊã†„ÇíÁµ±Âêà„Åó„ÄÅ„ÅÇ„Å™„Åü„ÅÆÊÄùËÄÉ„Éó„É≠„Çª„Çπ„ÇíÂπ≥Êòì„Å™Ë®ÄËëâ„ÅßË™¨Êòé„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
        """
        try:
            snn_llm = self.langchain_adapter
            explanation = snn_llm._call(prompt)
            return explanation
        except Exception as e:
            logging.error(f"LLM„Å´„Çà„ÇãËá™Â∑±Ë®ÄÂèä„ÅÆÁîüÊàê„Å´Â§±Êïó„Åó„Åæ„Åó„Åü: {e}")
            return "„Ç®„É©„Éº: Ëá™Â∑±Ë®ÄÂèä„ÅÆÁîüÊàê„Å´Â§±Êïó„Åó„Åæ„Åó„Åü„ÄÇ"
