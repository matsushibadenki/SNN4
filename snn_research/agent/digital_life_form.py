# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/agent/digital_life_form.py
# (æ›´æ–°)
# æ”¹å–„ç‚¹:
# - DigitalLifeFormãŒHierarchicalPlannerã‚’ç›´æ¥åˆ©ç”¨ã—ã¦ã€
#   é«˜ãƒ¬ãƒ™ãƒ«ã®ç›®æ¨™ã‹ã‚‰å…·ä½“çš„ãªè¡Œå‹•è¨ˆç”»ã‚’ç«‹æ¡ˆãƒ»å®Ÿè¡Œã™ã‚‹ã‚ˆã†ã«ä¿®æ­£ã€‚
# - `_decide_next_action`ã‚’`_formulate_goal`ã«æ”¹åã—ã€è‡ªç„¶è¨€èªã®ç›®æ¨™ã‚’ç”Ÿæˆã™ã‚‹ã‚ˆã†ã«ã—ãŸã€‚
# - `life_cycle_step`ã§ã€ç”Ÿæˆã•ã‚ŒãŸè¨ˆç”»ã‚’ãƒ«ãƒ¼ãƒ—å®Ÿè¡Œã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè£…ã€‚

import time
import logging
import torch
import random
import json
import asyncio
from typing import Dict, Any, Optional, List, TYPE_CHECKING
import operator
import os

from snn_research.cognitive_architecture.intrinsic_motivation import IntrinsicMotivationSystem
from snn_research.cognitive_architecture.meta_cognitive_snn import MetaCognitiveSNN
from snn_research.agent.memory import Memory
from snn_research.cognitive_architecture.physics_evaluator import PhysicsEvaluator
from snn_research.cognitive_architecture.symbol_grounding import SymbolGrounding
from snn_research.agent.autonomous_agent import AutonomousAgent
from snn_research.agent.reinforcement_learner_agent import ReinforcementLearnerAgent
from snn_research.agent.self_evolving_agent import SelfEvolvingAgent
from snn_research.cognitive_architecture.global_workspace import GlobalWorkspace
# HierarchicalPlannerã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from snn_research.cognitive_architecture.hierarchical_planner import HierarchicalPlanner

if TYPE_CHECKING:
    from app.adapters.snn_langchain_adapter import SNNLangChainAdapter
    from snn_research.training.bio_trainer import BioRLTrainer
    from snn_research.rl_env.grid_world import GridWorldEnv

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class DigitalLifeForm:
    """
    ãƒ—ãƒ©ãƒ³ãƒŠãƒ¼ã¨é€£æºã—ã€ç›®æ¨™ã«åŸºã¥ã„ãŸè¨ˆç”»ã‚’å®Ÿè¡Œã™ã‚‹ã€é€²åŒ–ã—ãŸã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ã€‚
    """
    def __init__(
        self,
        planner: HierarchicalPlanner,
        autonomous_agent: AutonomousAgent,
        rl_agent: ReinforcementLearnerAgent,
        self_evolving_agent: SelfEvolvingAgent,
        motivation_system: IntrinsicMotivationSystem,
        meta_cognitive_snn: MetaCognitiveSNN,
        memory: Memory,
        physics_evaluator: PhysicsEvaluator,
        symbol_grounding: SymbolGrounding,
        langchain_adapter: "SNNLangChainAdapter",
        global_workspace: GlobalWorkspace
    ):
        # --- â–¼ ä¿®æ­£ â–¼ ---
        self.planner = planner
        # --- â–² ä¿®æ­£ â–² ---
        self.autonomous_agent = autonomous_agent
        self.rl_agent = rl_agent
        self.self_evolving_agent = self_evolving_agent
        self.motivation_system = motivation_system
        self.meta_cognitive_snn = meta_cognitive_snn
        self.memory = memory
        self.physics_evaluator = physics_evaluator
        self.symbol_grounding = symbol_grounding
        self.langchain_adapter = langchain_adapter
        self.workspace = global_workspace
        
        self.running = False
        self.state: Dict[str, Any] = {"last_action": None, "last_result": None, "last_task": "unknown"}

    # ... (start, stop, life_cycleãƒ¡ã‚½ãƒƒãƒ‰ã¯å¤‰æ›´ãªã—) ...
    def start(self): self.running = True; logging.info("DigitalLifeForm activated."); self.life_cycle()
    def stop(self): self.running = False; logging.info("DigitalLifeForm deactivating.")
    def life_cycle(self):
        while self.running: self.life_cycle_step(); time.sleep(10)

    # --- â–¼ ä¿®æ­£ â–¼ ---
    def life_cycle_step(self):
        """è¨ˆç”»ä¸»å°ã®èªçŸ¥ã‚µã‚¤ã‚¯ãƒ«ã‚’å®Ÿè¡Œã™ã‚‹ã€‚"""
        logging.info("\n--- ğŸ§  New Cognitive Cycle ---")
        self._handle_causal_credit()

        # 1. å†…éƒ¨çŠ¶æ…‹ã‚’è©•ä¾¡ã—ã€é«˜ãƒ¬ãƒ™ãƒ«ã®ç›®æ¨™ã‚’ç­–å®šã™ã‚‹
        internal_state = self.motivation_system.get_internal_state()
        performance_eval = self.meta_cognitive_snn.evaluate_performance()
        goal = self._formulate_goal(internal_state, performance_eval)
        logging.info(f"ğŸ¯ New Goal: {goal}")
        
        # 2. ãƒ—ãƒ©ãƒ³ãƒŠãƒ¼ã«ç›®æ¨™ã‚’æ¸¡ã—ã€è¡Œå‹•è¨ˆç”»ã‚’ç«‹æ¡ˆã•ã›ã‚‹
        plan = asyncio.run(self.planner.create_plan(goal))

        # 3. è¨ˆç”»ã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ã‚’é †ç•ªã«å®Ÿè¡Œã™ã‚‹
        if not plan.task_list:
            logging.warning("Planner could not create a plan. Idling for this cycle.")
            return

        logging.info(f"ğŸ“‹ Plan Created: {[task.get('task') for task in plan.task_list]}")
        for task in plan.task_list:
            action = task.get('task')
            if not action: continue

            logging.info(f"â–¶ï¸ Executing task from plan: {action}")
            result, reward, expert_used = self._execute_action(action, internal_state)

            # 4. å„ã‚¹ãƒ†ãƒƒãƒ—ã®çµæœã‚’è¨˜éŒ²ãƒ»è©•ä¾¡ã™ã‚‹
            if isinstance(result, dict): self.symbol_grounding.process_observation(result, context=f"action '{action}'")
            reward_vector = {"external": reward, "curiosity": internal_state.get("curiosity", 0.0)}
            decision_context = {"goal": goal, "plan": [t.get('task') for t in plan.task_list]}
            self.memory.record_experience(self.state, action, result, reward, expert_used, decision_context)
            
            # (ç°¡æ˜“çš„ãªå‹•æ©Ÿæ›´æ–°)
            self.motivation_system.update_metrics(random.random(), 1.0 if reward > 0 else 0.0, random.random(), random.random())

            self.state["last_action"] = action; self.state["last_result"] = result
            logging.info(f"  - Task Result: {str(result)[:100]}, Reward: {reward:.2f}")

            if reward < 0:
                logging.warning(f"  - Task '{action}' failed. Aborting current plan.")
                break # è¨ˆç”»ã®é€”ä¸­ã§å¤±æ•—ã—ãŸã‚‰ä¸­æ­¢

    def _formulate_goal(self, internal_state: Dict[str, Any], performance_eval: Dict[str, Any]) -> str:
        """å†…éƒ¨çŠ¶æ…‹ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡ã‹ã‚‰ã€è‡ªç„¶è¨€èªã®ç›®æ¨™ã‚’ç”Ÿæˆã™ã‚‹ã€‚"""
        if internal_state.get("curiosity", 0.0) > 0.8 and internal_state.get("curiosity_context"):
            topic = internal_state.get("curiosity_context")
            return f"Explore the unknown concept related to '{str(topic)}' to satisfy curiosity."
        
        if performance_eval.get("status") == "capability_gap":
            return "Evolve my architecture to overcome a capability gap."

        if internal_state.get("boredom", 0.0) > 0.7:
            return "Explore a completely new and random task to alleviate boredom."

        return "Practice an existing skill to improve confidence and performance."

    # --- â–¼ ä¿®æ­£ â–¼ ---
    def _handle_causal_credit(self):
        """GlobalWorkspaceã‚’ç›£è¦–ã—ã€å› æœçš„ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆä¿¡å·ãŒã‚ã‚Œã°å‡¦ç†ã™ã‚‹ã€‚"""
        # Workspaceã‹ã‚‰æ„è­˜ã«ä¸Šã£ãŸæœ€æ–°ã®æƒ…å ±ã‚’å–å¾—
        conscious_content = self.workspace.conscious_broadcast_content
        
        if conscious_content and isinstance(conscious_content, dict) and conscious_content.get("type") == "causal_credit":
            target_action = conscious_content.get("target_action")
            credit = conscious_content.get("credit", 0.0)
            
            print(f"âœ¨ å› æœçš„ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆä¿¡å·ã‚’æ¤œçŸ¥ï¼ Target: {target_action}, Credit: {credit}")

            # ç›´è¿‘ã®è¡Œå‹•ãŒã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã®å¯¾è±¡ã¨ä¸€è‡´ã™ã‚‹ã‹ç¢ºèª
            if self.state.get("last_action") and target_action == f"action_{self.state['last_action']}":
                print(f"  - ç›´è¿‘ã®è¡Œå‹• '{self.state['last_action']}' ã«ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚’å‰²ã‚Šå½“ã¦ã¾ã™ã€‚")
                
                # ã“ã“ã§ã€ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚’é©åˆ‡ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å­¦ç¿’å‰‡ã«æ¸¡ã™
                # ã“ã®ä¾‹ã§ã¯ã€RLã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå¯¾è±¡ã ã¨ä»®å®š
                if self.state['last_action'] in ["explore_new_task_with_rl", "practice_skill_with_rl"]:
                    # learnãƒ¡ã‚½ãƒƒãƒ‰ã«ç‰¹åˆ¥ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¸¡ã™
                    self.rl_agent.learn(reward=0.0, causal_credit=credit)
                    print("  - RLã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚·ãƒŠãƒ—ã‚¹å¯å¡‘æ€§ã‚’ãƒˆãƒƒãƒ—ãƒ€ã‚¦ãƒ³ã§å¤‰èª¿ã—ã¾ã—ãŸã€‚")

    def _decide_next_action(self, internal_state: Dict[str, Any], performance_eval: Dict[str, Any]) -> str:
        # (å¤‰æ›´ãªã—)
        action_scores: Dict[str, float] = { "explore_curiosity": internal_state.get("curiosity", 0.0) * 20.0, "evolve_architecture": 0.0, "practice_skill_with_rl": internal_state.get("confidence", 0.5) * 2.0 }
        if performance_eval.get("status") == "capability_gap": action_scores["evolve_architecture"] += 10.0
        if internal_state.get("boredom", 0.0) > 0.8: action_scores["explore_curiosity"] += internal_state.get("boredom", 0.0) * 15.0
        chosen_action = max(action_scores.items(), key=operator.itemgetter(1))[0]
        logging.info(f"Action scores: {action_scores} -> Chosen: {chosen_action}")
        return chosen_action
    # --- â–² ä¿®æ­£ â–² ---

    def _execute_action(self, action: str, internal_state: Dict[str, Any]) -> tuple[Dict[str, Any], float, List[str]]:
        # (å¤‰æ›´ãªã—)
        from snn_research.rl_env.grid_world import GridWorldEnv
        from snn_research.training.bio_trainer import BioRLTrainer
        try:
            if action == "explore_curiosity":
                curiosity_topic = internal_state.get("curiosity_context")
                if not curiosity_topic: return {"status": "skipped", "info": "No specific curiosity context found."}, 0.0, []
                topic_str = str(curiosity_topic.get("action", "AI concept"))
                logging.info(f"ğŸ”¬ Curiosity triggered! Researching topic: '{topic_str}'")
                new_model_info = asyncio.run(self.autonomous_agent.handle_task(task_description=topic_str, unlabeled_data_path="data/sample_data.jsonl", force_retrain=True))
                if new_model_info: return {"status": "success", "info": f"Learned about '{topic_str}' and created new expert.", "model_info": new_model_info}, 1.0, ["autonomous_agent"]
                else: return {"status": "failure", "info": f"Failed to learn about '{topic_str}'."}, -0.5, ["autonomous_agent"]
            elif action == "evolve_architecture":
                return {"status": "success", "info": self.self_evolving_agent.evolve()}, 0.9, ["self_evolver"]
            elif action == "practice_skill_with_rl":
                env = GridWorldEnv(size=5, max_steps=20, device=self.rl_agent.device)
                trainer = BioRLTrainer(agent=self.rl_agent, env=env)
                res = trainer.train(num_episodes=10)
                return {"status": "success", "results": res}, res.get("final_average_reward", 0.0), ["rl_agent"]
            else: return {"status": "idle", "info": "No compelling action to take."}, 0.0, []
        except Exception as e:
            logging.error(f"Error executing action '{action}': {e}")
            return {"status": "error", "info": str(e)}, -1.0, []

    def awareness_loop(self, cycles: int):
        # (å¤‰æ›´ãªã—)
        print(f"ğŸ§¬ Digital Life Form awareness loop starting for {cycles} cycles.")
        self.running = True
        for i in range(cycles):
            if not self.running: break
            print(f"\n----- Cycle {i+1}/{cycles} -----")
            self.life_cycle_step()
            time.sleep(2)
        print("ğŸ§¬ Awareness loop finished.")

    def explain_last_action(self) -> Optional[str]:
        try:
            with open(self.memory.memory_path, "rb") as f:
                try:
                    f.seek(-2, os.SEEK_END)
                    while f.read(1) != b'\n': f.seek(-2, os.SEEK_CUR)
                except OSError: f.seek(0)
                last_line = f.readline().decode()
            last_experience = json.loads(last_line)
        except (IOError, json.JSONDecodeError, IndexError):
            return "è¡Œå‹•å±¥æ­´ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"

        prompt = f"""
        ã‚ãªãŸã¯ã€è‡ªèº«ã®è¡Œå‹•ã‚’åˆ†æã—ã€ãã®ç†ç”±ã‚’åˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜ã™ã‚‹AIã§ã™ã€‚
        ä»¥ä¸‹ã®å†…éƒ¨ãƒ­ã‚°ã¯ã€ã‚ãªãŸè‡ªèº«ã®ç›´è¿‘ã®è¡Œå‹•è¨˜éŒ²ã§ã™ã€‚ã“ã®è¨˜éŒ²ã‚’åŸºã«ã€ãªãœãã®è¡Œå‹•ã‚’å–ã£ãŸã®ã‹ã‚’ä¸€äººç§°ï¼ˆã€Œç§ã€ï¼‰ã§èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

        ### è¡Œå‹•ãƒ­ã‚°
        - **å®Ÿè¡Œã—ãŸè¡Œå‹•:** {last_experience.get('action')}
        - **æ„æ€æ±ºå®šã®æ ¹æ‹ :**
          - **å†…ç™ºçš„å‹•æ©Ÿï¼ˆå†…éƒ¨çŠ¶æ…‹ï¼‰:** {last_experience.get('decision_context', {}).get('internal_state')}
          - **è‡ªå·±ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡:** {last_experience.get('decision_context', {}).get('performance_eval')}
          - **ç‰©ç†åŠ¹ç‡è©•ä¾¡:** {last_experience.get('decision_context', {}).get('physical_rewards')}

        ### æŒ‡ç¤º
        ä¸Šè¨˜ã®æ ¹æ‹ ã‚’çµ±åˆã—ã€ã‚ãªãŸã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’å¹³æ˜“ãªè¨€è‘‰ã§èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
        """
        try:
            snn_llm = self.langchain_adapter
            explanation = snn_llm._call(prompt)
            return explanation
        except Exception as e:
            logging.error(f"LLMã«ã‚ˆã‚‹è‡ªå·±è¨€åŠã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return "ã‚¨ãƒ©ãƒ¼: è‡ªå·±è¨€åŠã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
