# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/agent/self_evolving_agent.py
# (æ›´æ–°)
#
# Title: è‡ªå·±é€²åŒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
#
# æ”¹å–„ç‚¹ (v2): ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—ã€Œå­¦ç¿’ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã®è‡ªå·±é€²åŒ–ã€ã‚’å®Ÿè£…ã€‚
#              ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚„å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã ã‘ã§ãªãã€å­¦ç¿’ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è‡ªä½“
#              (ä¾‹: 'bio-causal-sparse' -> 'bio-particle-filter') ã‚’
#              è‡ªå¾‹çš„ã«å¤‰æ›´ã™ã‚‹`_evolve_learning_paradigm`ãƒ¡ã‚½ãƒƒãƒ‰ã‚’è¿½åŠ ã€‚

from typing import Dict, Any, Optional
import os
import yaml
from omegaconf import OmegaConf
import random

from .autonomous_agent import AutonomousAgent
from snn_research.cognitive_architecture.hierarchical_planner import HierarchicalPlanner
from snn_research.distillation.model_registry import ModelRegistry
from snn_research.tools.web_crawler import WebCrawler
from .memory import Memory as AgentMemory


class SelfEvolvingAgent(AutonomousAgent):
    """
    è‡ªèº«ã®æ€§èƒ½ã‚’ç›£è¦–ã—ã€å¿…è¦ã«å¿œã˜ã¦è‡ªå·±é€²åŒ–ã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€‚
    """
    def __init__(
        self,
        name: str,
        planner: HierarchicalPlanner,
        model_registry: ModelRegistry,
        memory: AgentMemory,
        web_crawler: WebCrawler,
        evolution_threshold: float = 0.5,
        project_root: str = ".",
        model_config_path: Optional[str] = None,
        training_config_path: Optional[str] = None,
    ):
        super().__init__(name, planner, model_registry, memory, web_crawler)
        self.evolution_threshold = evolution_threshold
        self.project_root = project_root
        self.model_config_path = model_config_path
        self.training_config_path = training_config_path


    def execute(self, task_description: str) -> str:
        """
        ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã—ã€çµæœã‚’è©•ä¾¡ã—ã¦è‡ªå·±é€²åŒ–ã‚’è©¦ã¿ã‚‹ã€‚
        """
        result = super().execute(task_description)
        
        performance = self.evaluate_performance(task_description, result)
        
        if performance < self.evolution_threshold:
            print(f"ğŸ“‰ Performance ({performance:.2f}) is below threshold ({self.evolution_threshold}). Triggering evolution...")
            evolution_result = self.evolve()
            return f"{result}\nAdditionally, self-evolution was triggered: {evolution_result}"

        return result

    def evaluate_performance(self, task: str, result: str) -> float:
        """
        ã‚¿ã‚¹ã‚¯ã®å®Ÿè¡Œçµæœã‚’è©•ä¾¡ã™ã‚‹ï¼ˆç°¡æ˜“ç‰ˆï¼‰ã€‚
        """
        if "successfully" in result.lower() or "using expert" in result.lower():
            return 0.9
        if "no specific expert found" in result.lower():
            return 0.4
        return 0.1

    def evolve(self) -> str:
        """
        è‡ªå·±é€²åŒ–ã®ãƒ—ãƒ­ã‚»ã‚¹ã‚’å®Ÿè¡Œã™ã‚‹ã€‚
        ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€å­¦ç¿’ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã®ã„ãšã‚Œã‹ã‚’é€²åŒ–ã•ã›ã‚‹ã€‚
        """
        # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†“ä¿®æ­£é–‹å§‹â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
        evolution_choice = random.random()
        if evolution_choice < 0.5:  # 50%ã®ç¢ºç‡ã§ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’é€²åŒ–
            return self._evolve_architecture()
        elif evolution_choice < 0.8: # 30%ã®ç¢ºç‡ã§å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é€²åŒ–
            return self._evolve_learning_parameters()
        else: # 20%ã®ç¢ºç‡ã§å­¦ç¿’ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è‡ªä½“ã‚’é€²åŒ–
            return self._evolve_learning_paradigm()
        # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†‘ä¿®æ­£çµ‚ã‚ã‚Šâ—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸

    def _evolve_architecture(self) -> str:
        """ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’é€²åŒ–ã•ã›ã‚‹ã€‚"""
        if not self.model_config_path or not os.path.exists(self.model_config_path):
            return "Architecture evolution failed: model_config_path is not set or file not found."

        try:
            print(f"ğŸ§¬ Starting architecture evolution for {self.model_config_path}...")
            
            cfg = OmegaConf.load(self.model_config_path)

            original_d_model = cfg.model.get("d_model", 128)
            original_num_layers = cfg.model.get("num_layers", 4)

            cfg.model.d_model = int(original_d_model * random.uniform(1.1, 1.5))
            cfg.model.num_layers = original_num_layers + random.randint(1, 2)
            
            if 'd_state' in cfg.model:
                cfg.model.d_state = int(cfg.model.d_state * 1.5)
            if 'neuron' in cfg.model and 'branch_features' in cfg.model.neuron:
                cfg.model.neuron.branch_features = cfg.model.d_model // cfg.model.neuron.get("num_branches", 4)

            print(f"   - d_model evolved: {original_d_model} -> {cfg.model.d_model}")
            print(f"   - num_layers evolved: {original_num_layers} -> {cfg.model.num_layers}")

            base_name, ext = os.path.splitext(self.model_config_path)
            new_config_path = f"{base_name}_evolved_v{self.get_next_version()}{ext}"
            
            OmegaConf.save(config=cfg, f=new_config_path)

            return f"Successfully evolved architecture. New configuration saved to '{new_config_path}'."

        except Exception as e:
            return f"Architecture evolution failed with an error: {e}"

    def _evolve_learning_parameters(self) -> str:
        """å­¦ç¿’ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é€²åŒ–ã•ã›ã‚‹ã€‚"""
        if not self.training_config_path or not os.path.exists(self.training_config_path):
            return "Learning parameter evolution failed: training_config_path is not set or file not found."

        try:
            print(f"ğŸ§  Starting learning parameter evolution for {self.training_config_path}...")
            cfg = OmegaConf.load(self.training_config_path)

            params_to_evolve = [
                "training.gradient_based.learning_rate",
                "training.gradient_based.loss.spike_reg_weight",
                "training.gradient_based.loss.mem_reg_weight",
                "training.biologically_plausible.stdp.learning_rate",
                "training.biologically_plausible.stdp.tau_trace",
            ]
            
            param_key = random.choice(params_to_evolve)
            
            selected_param = OmegaConf.select(cfg, param_key)
            if selected_param is None:
                return f"Parameter '{param_key}' not found in the config. Skipping evolution."

            original_value = selected_param
            new_value = original_value * random.uniform(0.8, 1.2)
            
            OmegaConf.update(cfg, param_key, new_value, merge=True)

            print(f"   - Evolved parameter '{param_key}': {original_value:.6f} -> {new_value:.6f}")

            base_name, ext = os.path.splitext(self.training_config_path)
            new_config_path = f"{base_name}_evolved_v{self.get_next_version()}{ext}"
            OmegaConf.save(config=cfg, f=new_config_path)

            return f"Successfully evolved learning parameters. New configuration saved to '{new_config_path}'."

        except Exception as e:
            return f"Learning parameter evolution failed with an error: {e}"

    # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†“è¿½åŠ é–‹å§‹â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
    def _evolve_learning_paradigm(self) -> str:
        """å­¦ç¿’ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è‡ªä½“ã‚’é€²åŒ–ã•ã›ã‚‹ã€‚"""
        if not self.training_config_path or not os.path.exists(self.training_config_path):
            return "Learning paradigm evolution failed: training_config_path is not set or file not found."

        try:
            print(f"ğŸ”„ Starting learning paradigm evolution for {self.training_config_path}...")
            cfg = OmegaConf.load(self.training_config_path)

            current_paradigm = cfg.training.get("paradigm", "gradient_based")
            
            # åˆ©ç”¨å¯èƒ½ãªä»£æ›¿ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã®ãƒªã‚¹ãƒˆ
            available_paradigms = ["bio-causal-sparse", "bio-particle-filter", "physics_informed"]
            
            # ç¾åœ¨ã®ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚’é™¤å¤–ã—ãŸãƒªã‚¹ãƒˆã‹ã‚‰æ–°ã—ã„ã‚‚ã®ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠ
            new_paradigm = random.choice([p for p in available_paradigms if p != current_paradigm])

            cfg.training.paradigm = new_paradigm
            
            print(f"   - Learning paradigm evolved: '{current_paradigm}' -> '{new_paradigm}'")
            
            # æ–°ã—ã„è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
            base_name, ext = os.path.splitext(self.training_config_path)
            new_config_path = f"{base_name}_evolved_v{self.get_next_version()}{ext}"
            OmegaConf.save(config=cfg, f=new_config_path)

            return f"Successfully evolved learning paradigm. New configuration saved to '{new_config_path}'."

        except Exception as e:
            return f"Learning paradigm evolution failed with an error: {e}"
    # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†‘è¿½åŠ çµ‚ã‚ã‚Šâ—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸

    def get_next_version(self) -> int:
        # ç°¡æ˜“çš„ãªãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†
        return random.randint(100, 999)

    def run_evolution_cycle(self, task_description: str, initial_metrics: Dict[str, float]) -> None:
        """snn-cli.pyã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹ãŸã‚ã®ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆã€‚"""
        print(f"Running evolution cycle for task: {task_description} with initial metrics: {initial_metrics}")
        
        performance = initial_metrics.get("accuracy", 0.0)
        
        if performance < self.evolution_threshold:
            print(f"ğŸ“‰ Initial performance ({performance:.2f}) is below threshold ({self.evolution_threshold}).")
            evolution_result = self.evolve()
            print(f"âœ¨ {evolution_result}")
        else:
            print(f"âœ… Initial performance ({performance:.2f}) is sufficient. No evolution needed.")