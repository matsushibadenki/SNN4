# matsushibadenki/snn4/snn_research/cognitive_architecture/rag_snn.py
#
# Phase 3: RAG-SNN (Retrieval-Augmented Generation) ã‚·ã‚¹ãƒ†ãƒ 
#
# æ”¹å–„ç‚¹:
# - ROADMAPãƒ•ã‚§ãƒ¼ã‚º7ã«åŸºã¥ãã€ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã¨ã—ã¦ã®æ©Ÿèƒ½ã‚’è¿½åŠ ã€‚
# - add_relationshipãƒ¡ã‚½ãƒƒãƒ‰ã‚’å®Ÿè£…ã—ã€æ¦‚å¿µé–“ã®é–¢ä¿‚æ€§ã‚’
#   æ§‹é€ åŒ–ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«è¿½åŠ ã§ãã‚‹ã‚ˆã†ã«ã—ãŸã€‚
#
# æ”¹å–„ç‚¹ (v2):
# - ROADMAPãƒ•ã‚§ãƒ¼ã‚º3ã€Œå› æœãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã€ã«åŸºã¥ãã€
#   `add_causal_relationship`ãƒ¡ã‚½ãƒƒãƒ‰ã‚’è¿½åŠ ã€‚
#   ã“ã‚Œã«ã‚ˆã‚Šã€ã€ŒA causes Bã€ã®ã‚ˆã†ãªå› æœé–¢ä¿‚ã‚’ã‚ˆã‚Šæ˜ç¢ºã«è¡¨ç¾ã§ãã‚‹ã‚ˆã†ã«ãªã‚‹ã€‚
#
# ä¿®æ­£ç‚¹ (v3):
# - TypeErrorã‚’è§£æ¶ˆã™ã‚‹ãŸã‚ã€vector_store_pathãŒNoneã®å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã‚’è¿½åŠ ã€‚
# - mypyã‚¨ãƒ©ãƒ¼ã‚’è§£æ¶ˆã™ã‚‹ãŸã‚ã€vector_store_pathã«å‹ãƒ’ãƒ³ãƒˆã‚’è¿½åŠ ã€‚

import os
from typing import List, Optional
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import DirectoryLoader, TextLoader
# HuggingFaceEmbeddings ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆå…ƒã‚’å¤‰æ›´
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

class RAGSystem:
    """
    å¤–éƒ¨çŸ¥è­˜ï¼ˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼‰ã¨å†…éƒ¨è¨˜æ†¶ï¼ˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ­ã‚°ï¼‰ã‚’æ¤œç´¢ã—ã€
    æ€è€ƒã®ãŸã‚ã®æ–‡è„ˆã‚’æä¾›ã™ã‚‹RAGã‚·ã‚¹ãƒ†ãƒ ã€‚
    ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã¨ã—ã¦ã®æ©Ÿèƒ½ã‚‚ä½µã›æŒã¤ã€‚
    """
    def __init__(self, vector_store_path: Optional[str] = "runs/vector_store"):
        if vector_store_path is None:
            print("âš ï¸ RAGSystemã«Noneã®ãƒ‘ã‚¹ãŒæ¸¡ã•ã‚ŒãŸãŸã‚ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ 'runs/vector_store' ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            self.vector_store_path: str = "runs/vector_store"
        else:
            self.vector_store_path = vector_store_path
        self.embedding_model = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2"
        )
        self.vector_store: Optional[FAISS] = self._load_vector_store()

    def _load_vector_store(self) -> Optional[FAISS]:
        """ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ãƒ‡ã‚£ã‚¹ã‚¯ã‹ã‚‰èª­ã¿è¾¼ã‚€ã€‚"""
        if os.path.exists(self.vector_store_path):
            print(f"ğŸ“š æ—¢å­˜ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {self.vector_store_path}")
            return FAISS.load_local(self.vector_store_path, self.embedding_model, allow_dangerous_deserialization=True)
        return None

    def setup_vector_store(self, knowledge_dir: str = "doc", memory_file: str = "runs/agent_memory.jsonl"):
        """
        çŸ¥è­˜æºã‹ã‚‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿ã€ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’æ§‹ç¯‰ãƒ»ä¿å­˜ã™ã‚‹ã€‚
        """
        print("ğŸ› ï¸ ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®æ§‹ç¯‰ã‚’é–‹å§‹ã—ã¾ã™...")
        
        # 1. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆ.md, .txtï¼‰ã‚’èª­ã¿è¾¼ã‚€
        doc_loader = DirectoryLoader(
            knowledge_dir, glob="**/*.md", loader_cls=TextLoader, silent_errors=True
        )
        txt_loader = DirectoryLoader(
            knowledge_dir, glob="**/*.txt", loader_cls=TextLoader, silent_errors=True
        )
        docs = doc_loader.load() + txt_loader.load()

        # 2. ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¨˜æ†¶ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        if os.path.exists(memory_file):
            memory_loader = TextLoader(memory_file)
            docs.extend(memory_loader.load())
        
        if not docs:
            print("âš ï¸ çŸ¥è­˜æºã¨ãªã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return

        # 3. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’åˆ†å‰²
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        split_docs = text_splitter.split_documents(docs)

        # 4. ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¦FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ
        print(f"ğŸ“„ {len(split_docs)}å€‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒãƒ£ãƒ³ã‚¯ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¦ã„ã¾ã™...")
        self.vector_store = FAISS.from_documents(split_docs, self.embedding_model)
        
        # 5. ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ãƒ‡ã‚£ã‚¹ã‚¯ã«ä¿å­˜
        os.makedirs(os.path.dirname(self.vector_store_path), exist_ok=True)
        self.vector_store.save_local(self.vector_store_path)
        print(f"âœ… ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®æ§‹ç¯‰ãŒå®Œäº†ã—ã€'{self.vector_store_path}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

    def search(self, query: str, k: int = 3) -> List[str]:
        """
        ã‚¯ã‚¨ãƒªã«æœ€ã‚‚é–¢é€£ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒãƒ£ãƒ³ã‚¯ã‚’æ¤œç´¢ã™ã‚‹ã€‚
        """
        if self.vector_store is None:
            print("ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ãŒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å…ˆã« setup_vector_store() ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
            # ãƒ©ã‚¤ãƒ–ã§ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚’è©¦ã¿ã‚‹
            self.setup_vector_store()
            if self.vector_store is None:
                 return ["ã‚¨ãƒ©ãƒ¼: ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’æ§‹ç¯‰ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"]

        results = self.vector_store.similarity_search(query, k=k)
        return [doc.page_content for doc in results]

    def add_relationship(self, source_concept: str, relation: str, target_concept: str):
        """
        æ¦‚å¿µé–“ã®é–¢ä¿‚æ€§ã‚’ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ï¼ˆãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ï¼‰ã«è¿½åŠ ã™ã‚‹ã€‚
        """
        if self.vector_store is None:
            print("âš ï¸ ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚æ–°è¦ä½œæˆã—ã¾ã™ã€‚")
            self.vector_store = FAISS.from_texts([], self.embedding_model)

        # é–¢ä¿‚æ€§ã‚’æ§‹é€ åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦è¡¨ç¾
        relationship_text = f"Concept Relation: {source_concept} {relation} {target_concept}."
        
        # LangChainã®Documentã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨ã—ã¦è¿½åŠ 
        doc = Document(page_content=relationship_text, metadata={"source": "internal_knowledge"})
        self.vector_store.add_documents([doc])
        
        # æ›´æ–°ã‚’ãƒ‡ã‚£ã‚¹ã‚¯ã«ä¿å­˜
        self.vector_store.save_local(self.vector_store_path)
        print(f"ğŸ“ˆ ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•æ›´æ–°: ã€Œ{relationship_text}ã€")

    def add_causal_relationship(self, cause: str, effect: str, condition: Optional[str] = None):
        """
        æ¦‚å¿µé–“ã®å› æœé–¢ä¿‚ã‚’ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã«è¿½åŠ ã™ã‚‹ã€‚
        """
        if self.vector_store is None:
            self.setup_vector_store()
            if self.vector_store is None:
                self.vector_store = FAISS.from_texts([], self.embedding_model)

        if condition:
            causal_text = f"Causal Relation: Under condition '{condition}', the event '{cause}' leads to the effect '{effect}'."
        else:
            causal_text = f"Causal Relation: The event '{cause}' directly leads to the effect '{effect}'."
        
        doc = Document(page_content=causal_text, metadata={"source": "causal_inference"})
        self.vector_store.add_documents([doc])
        self.vector_store.save_local(self.vector_store_path)
        print(f"ğŸ”— å› æœé–¢ä¿‚ã‚’è¨˜éŒ²: ã€Œ{causal_text}ã€")