# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/cognitive_architecture/basal_ganglia.py
# ã‚¿ã‚¤ãƒˆãƒ«: å¤§è„³åŸºåº•æ ¸ï¼šæƒ…å‹•å¤‰èª¿ã‚’ä¼´ã†è¡Œå‹•é¸æŠãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
# æ©Ÿèƒ½èª¬æ˜:
# - è„³ã®ç›´æ¥è·¯ï¼ˆGoï¼‰ã¨é–“æ¥è·¯ï¼ˆNoGoï¼‰ã®æ©Ÿèƒ½ã‚’æ¨¡å€£ã—ã€è¤‡æ•°ã®é¸æŠè‚¢ã‹ã‚‰æœ€é©ãªè¡Œå‹•ã‚’æ±ºå®šã™ã‚‹ã€‚
# - Amygdalaã‹ã‚‰å—ã‘å–ã£ãŸæƒ…å‹•ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆå¿«ãƒ»ä¸å¿«ã€è¦šé†’ãƒ»æ²ˆé™ï¼‰ã«åŸºã¥ãã€
#   æ„æ€æ±ºå®šã®é–¾å€¤ã‚’å‹•çš„ã«èª¿æ•´ã™ã‚‹ã€‚ä¾‹ãˆã°ã€å±é™ºã‚’å¯ŸçŸ¥ã—ãŸå ´åˆï¼ˆä¸å¿«ãƒ»é«˜è¦šé†’ï¼‰ã€
#   ã‚ˆã‚Šè¿…é€Ÿã«è¡Œå‹•ã‚’èµ·ã“ã›ã‚‹ã‚ˆã†ã«é–¾å€¤ã‚’ä¸‹ã’ã‚‹ã€‚
# - å®Ÿè¡Œãƒ­ã‚°ã‚’å¼·åŒ–ã—ã€æƒ…å‹•ãŒæ„æ€æ±ºå®šã«ä¸ãˆãŸå½±éŸ¿ã‚’æ˜ç¢ºã«è¡¨ç¤ºã™ã‚‹ã‚ˆã†ã«ã—ãŸã€‚

from typing import List, Dict, Any, Optional
import torch
import torch.nn.functional as F

class BasalGanglia:
    """
    ä¾¡å€¤ä¿¡å·ã¨æƒ…å‹•æ–‡è„ˆã«åŸºã¥ã„ã¦è¡Œå‹•é¸æŠã‚’è¡Œã†å¤§è„³åŸºåº•æ ¸ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€‚
    """
    def __init__(self, selection_threshold: float = 0.5, inhibition_strength: float = 0.3):
        """
        Args:
            selection_threshold (float): è¡Œå‹•ã‚’å®Ÿè¡Œã«ç§»ã™ãŸã‚ã®åŸºæœ¬çš„ãªæ´»æ€§åŒ–ãƒ¬ãƒ™ãƒ«ã€‚
            inhibition_strength (float): é¸æŠã•ã‚Œãªã‹ã£ãŸè¡Œå‹•ã«å¯¾ã™ã‚‹æŠ‘åˆ¶ã®å¼·ã•ã€‚
        """
        self.base_threshold = selection_threshold
        self.inhibition_strength = inhibition_strength
        print("ğŸ§  å¤§è„³åŸºåº•æ ¸ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒåˆæœŸåŒ–ã•ã‚Œã¾ã—ãŸã€‚")

    def _modulate_threshold(self, emotion_context: Optional[Dict[str, float]]) -> float:
        """æƒ…å‹•çŠ¶æ…‹ã«åŸºã¥ã„ã¦è¡Œå‹•é¸æŠã®é–¾å€¤ã‚’å‹•çš„ã«èª¿æ•´ã™ã‚‹ã€‚"""
        if emotion_context is None:
            return self.base_threshold

        valence = emotion_context.get("valence", 0.0)
        arousal = emotion_context.get("arousal", 0.0)
        
        # è¦šé†’åº¦ãŒé«˜ã„ã»ã©ã€é–¾å€¤ã¯ä¸‹ãŒã‚Šã€ã‚ˆã‚Šåå¿œçš„ã«ãªã‚‹
        # valenceãŒè² ï¼ˆä¸å¿«ï¼‰ã®å ´åˆã€ãã®åŠ¹æœã¯ã•ã‚‰ã«å¢—å¹…ã•ã‚Œã‚‹ (å±é™ºå›é¿ãªã©)
        arousal_effect = -arousal * 0.2
        valence_effect = -valence * arousal * 0.1 # ä¸å¿«ã§è¦šé†’åº¦ãŒé«˜ã„ã»ã©ã€ã•ã‚‰ã«é–¾å€¤ã‚’ä¸‹ã’ã‚‹
        
        modulated_threshold = self.base_threshold + arousal_effect + valence_effect
        
        # é–¾å€¤ãŒ0.1ã€œ0.9ã®ç¯„å›²ã«åã¾ã‚‹ã‚ˆã†ã«ã‚¯ãƒªãƒƒãƒ—
        final_threshold = max(0.1, min(0.9, modulated_threshold))
        
        if final_threshold != self.base_threshold:
            print(f"  - å¤§è„³åŸºåº•æ ¸: æƒ…å‹•ã«ã‚ˆã‚Šé–¾å€¤ã‚’èª¿æ•´ ({self.base_threshold:.2f} -> {final_threshold:.2f})")
        
        return final_threshold

    def select_action(
        self, 
        action_candidates: List[Dict[str, Any]],
        emotion_context: Optional[Dict[str, float]] = None
    ) -> Optional[Dict[str, Any]]:
        """
        æç¤ºã•ã‚ŒãŸè¡Œå‹•å€™è£œã®ä¸­ã‹ã‚‰ã€å®Ÿè¡Œã™ã¹ãæœ€é©ãªè¡Œå‹•ã‚’ä¸€ã¤é¸æŠã™ã‚‹ã€‚

        Args:
            action_candidates (List[Dict[str, Any]]):
                å„è¦ç´ ãŒè¡Œå‹•ã¨ãã®ä¾¡å€¤ã‚’æŒã¤è¾æ›¸ã®ãƒªã‚¹ãƒˆã€‚
                ä¾‹: [{'action': 'A', 'value': 0.9}, {'action': 'B', 'value': 0.6}]
            emotion_context (Optional[Dict[str, float]]):
                ç¾åœ¨ã®æƒ…å‹•çŠ¶æ…‹ã€‚ä¾‹: {'valence': -0.8, 'arousal': 0.9}

        Returns:
            Optional[Dict[str, Any]]: é¸æŠã•ã‚ŒãŸè¡Œå‹•ã€‚ã©ã®è¡Œå‹•ã‚‚é–¾å€¤ã«é”ã—ãªã„å ´åˆã¯Noneã€‚
        """
        if not action_candidates:
            print("ğŸ¤” å¤§è„³åŸºåº•æ ¸: è¡Œå‹•å€™è£œãŒæç¤ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
            return None
            
        current_threshold = self._modulate_threshold(emotion_context)

        # å„å€™è£œã®ä¾¡å€¤ã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
        values = torch.tensor([candidate.get('value', 0.0) for candidate in action_candidates])
        print(f"  - å¤§è„³åŸºåº•æ ¸: æ¤œè¨ä¸­ã®è¡Œå‹•å€™è£œ: {[c.get('action') for c in action_candidates]}, ä¾¡å€¤: {[round(v.item(), 2) for v in values]}")


        # 1. ç›´æ¥è·¯ (Go Pathway) ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³:
        #    æœ€ã‚‚ä¾¡å€¤ã®é«˜ã„è¡Œå‹•ã‚’ç‰¹å®šã™ã‚‹ã€‚
        winner_takes_all = F.softmax(values * 5.0, dim=0) # softmaxã§å‹è€…ã‚’å¼·èª¿

        # 2. é–“æ¥è·¯ (NoGo Pathway) ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³:
        #    ç«¶åˆã™ã‚‹è¡Œå‹•ã«å¯¾ã™ã‚‹æŠ‘åˆ¶ã‚’è¨ˆç®—ã™ã‚‹ã€‚
        #    ã“ã“ã§ã¯ã€å‹è€…ä»¥å¤–ã®æ´»æ€§ã‚’å¼±ã‚ã‚‹å½¢ã§ç°¡æ˜“çš„ã«è¡¨ç¾ã€‚
        inhibition_mask = torch.ones_like(values)
        winner_index = torch.argmax(values)
        # å‹è€…ä»¥å¤–ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«æŠ‘åˆ¶ã‚’é©ç”¨
        for i in range(len(inhibition_mask)):
            if i != winner_index:
                inhibition_mask[i] = 1.0 - self.inhibition_strength


        # æœ€çµ‚çš„ãªè¡Œå‹•æ´»æ€§ã‚’è¨ˆç®—
        final_activation = winner_takes_all * inhibition_mask

        # 3. æœ€çµ‚çš„ãªæ„æ€æ±ºå®š
        # æœ€ã‚‚æ´»æ€§ã®é«˜ã„è¡Œå‹•ãŒã€å®Ÿè¡Œé–¾å€¤ã‚’è¶…ãˆã¦ã„ã‚‹ã‹ç¢ºèª
        best_action_index = torch.argmax(final_activation)
        if final_activation[best_action_index] >= current_threshold:
            selected_action = action_candidates[best_action_index]
            print(f"ğŸ† è¡Œå‹•é¸æŠ: '{selected_action.get('action')}' (æ´»æ€§å€¤: {final_activation[best_action_index]:.2f}, é–¾å€¤: {current_threshold:.2f})")
            return selected_action
        else:
            print(f"ğŸ¤” è¡Œå‹•æ£„å´: ã©ã®è¡Œå‹•ã‚‚å®Ÿè¡Œé–¾å€¤ ({current_threshold:.2f}) ã«é”ã—ã¾ã›ã‚“ã§ã—ãŸã€‚(æœ€å¤§æ´»æ€§å€¤: {final_activation.max():.2f})")
            return None
