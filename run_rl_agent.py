# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: matsushibadenki/snn4/run_rl_agent.py
# Title: å¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
# Description: ç”Ÿç‰©å­¦çš„å­¦ç¿’å‰‡ã«åŸºã¥ãSNNã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’èµ·å‹•ã—ã€
#              å¼·åŒ–å­¦ç¿’ã®ãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
# æ”¹å–„ç‚¹:
# - ROADMAPãƒ•ã‚§ãƒ¼ã‚º2æ¤œè¨¼ã®ãŸã‚ã€GridWorldEnvã«å¯¾å¿œã€‚
# - ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè£…ã—ã€è¤‡æ•°ã‚¹ãƒ†ãƒƒãƒ—ã®ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«ã—ãŸã€‚
# æ”¹å–„ç‚¹ (v2):
# - ROADMAPãƒ•ã‚§ãƒ¼ã‚º2å®Œäº†ã®ãŸã‚ã€å­¦ç¿’çµæœã‚’å¯è¦–åŒ–ãƒ»ä¿å­˜ã™ã‚‹æ©Ÿèƒ½ã‚’è¿½åŠ ã€‚
# - å­¦ç¿’çµ‚äº†å¾Œã«å ±é…¬ã®æ¨ç§»ã‚’ã‚°ãƒ©ãƒ•ã¨ã—ã¦ãƒ—ãƒ­ãƒƒãƒˆã—ã€ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ã€‚
# - è¨“ç·´æ¸ˆã¿ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã€‚

import torch
import argparse
from tqdm import tqdm
import matplotlib.pyplot as plt
from pathlib import Path

from snn_research.agent.reinforcement_learner_agent import ReinforcementLearnerAgent
from snn_research.rl_env.grid_world import GridWorldEnv

def plot_rewards(rewards: list, save_path: Path):
    """å ±é…¬ã®æ¨ç§»ã‚’ãƒ—ãƒ­ãƒƒãƒˆã—ã¦ä¿å­˜ã™ã‚‹ã€‚"""
    plt.figure(figsize=(12, 6))
    plt.plot(rewards, label='Reward per Episode')
    # ç§»å‹•å¹³å‡ã‚’è¨ˆç®—ã—ã¦ãƒ—ãƒ­ãƒƒãƒˆ
    moving_avg = [sum(rewards[i-50:i]) / 50 for i in range(50, len(rewards))]
    plt.plot(range(50, len(rewards)), moving_avg, label='Moving Average (50 episodes)', color='red')
    plt.title('Reinforcement Learning Curve')
    plt.xlabel('Episode')
    plt.ylabel('Total Reward')
    plt.legend()
    plt.grid(True)
    plt.savefig(save_path)
    print(f"ğŸ“Š å­¦ç¿’æ›²ç·šã‚°ãƒ©ãƒ•ã‚’ '{save_path}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

def main():
    parser = argparse.ArgumentParser(description="Biologically Plausible Reinforcement Learning Framework")
    parser.add_argument("--episodes", type=int, default=1000, help="Number of learning episodes.")
    parser.add_argument("--grid_size", type=int, default=5, help="Size of the grid world.")
    parser.add_argument("--max_steps", type=int, default=50, help="Maximum steps per episode.")
    parser.add_argument("--output_dir", type=str, default="runs/rl_results", help="Directory to save results.")
    args = parser.parse_args()
    
    output_path = Path(args.output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    device = "mps" if torch.backends.mps.is_available() else "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")

    env = GridWorldEnv(size=args.grid_size, max_steps=args.max_steps, device=device)
    agent = ReinforcementLearnerAgent(input_size=4, output_size=4, device=device)

    print("\n" + "="*20 + "ğŸ¤– ç”Ÿç‰©å­¦çš„å¼·åŒ–å­¦ç¿’é–‹å§‹ (Grid World) ğŸ¤–" + "="*20)
    
    progress_bar = tqdm(range(args.episodes))
    total_rewards = []

    for episode in progress_bar:
        state = env.reset()
        done = False
        episode_reward = 0.0
        
        while not done:
            action = agent.get_action(state)
            next_state, reward, done = env.step(action)
            agent.learn(reward)
            episode_reward += reward
            state = next_state
        
        total_rewards.append(episode_reward)
        avg_reward = sum(total_rewards[-20:]) / len(total_rewards[-20:])
        
        progress_bar.set_description(f"Episode {episode+1}/{args.episodes}")
        progress_bar.set_postfix({"Last Reward": f"{episode_reward:.2f}", "Avg Reward (last 20)": f"{avg_reward:.3f}"})

    final_avg_reward = sum(total_rewards) / args.episodes if args.episodes > 0 else 0.0
    print("\n" + "="*20 + "âœ… å­¦ç¿’å®Œäº†" + "="*20)
    print(f"æœ€çµ‚çš„ãªå¹³å‡å ±é…¬: {final_avg_reward:.4f}")

    # å­¦ç¿’çµæœã®ãƒ—ãƒ­ãƒƒãƒˆã¨ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
    plot_rewards(total_rewards, output_path / "rl_learning_curve.png")
    
    model_save_path = output_path / "trained_rl_agent.pth"
    torch.save(agent.model.state_dict(), model_save_path)
    print(f"ğŸ’¾ è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ '{model_save_path}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")


if __name__ == "__main__":
    main()